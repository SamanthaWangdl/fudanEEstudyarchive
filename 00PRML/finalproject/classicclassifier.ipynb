{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入的库包括用于化学信息处理的rdkit库，还有sklearn的一些库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\mol2vec-master\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mol2vec.features import mol2alt_sentence, MolSentence, DfVec, sentences2vec\n",
    "from mol2vec.helpers import depict_identifier, plot_2D_vectors, IdentifierTable, mol_to_svg\n",
    "from gensim.models import word2vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve,recall_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.EState import Fingerprinter\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Descriptors\n",
    "import os\n",
    "import math\n",
    "#os.chdir('/admin/Downloads/mol2vec-master')\n",
    "%cd .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一个实验，对分子描述符提取的特征使用分类器分类，这里选用随机森林和mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22632262\n",
      "Iteration 2, loss = 0.09703120\n",
      "Iteration 3, loss = 0.06666071\n",
      "Iteration 4, loss = 0.05318610\n",
      "Iteration 5, loss = 0.04551671\n",
      "Iteration 6, loss = 0.03998555\n",
      "Iteration 7, loss = 0.03484627\n",
      "Iteration 8, loss = 0.03138832\n",
      "Iteration 9, loss = 0.02876949\n",
      "Iteration 10, loss = 0.02590400\n",
      "Iteration 11, loss = 0.02396547\n",
      "Iteration 12, loss = 0.02204197\n",
      "Iteration 13, loss = 0.02038440\n",
      "Iteration 14, loss = 0.01884410\n",
      "Iteration 15, loss = 0.01747396\n",
      "Iteration 16, loss = 0.01635332\n",
      "Iteration 17, loss = 0.01515711\n",
      "Iteration 18, loss = 0.01422090\n",
      "Iteration 19, loss = 0.01326010\n",
      "Iteration 20, loss = 0.01252532\n",
      "Iteration 21, loss = 0.01182362\n",
      "Iteration 22, loss = 0.01107983\n",
      "Iteration 23, loss = 0.01044050\n",
      "Iteration 24, loss = 0.00989231\n",
      "Iteration 25, loss = 0.00932359\n",
      "Iteration 26, loss = 0.00887018\n",
      "Iteration 27, loss = 0.00837451\n",
      "Iteration 28, loss = 0.00798907\n",
      "Iteration 29, loss = 0.00760758\n",
      "Iteration 30, loss = 0.00725308\n",
      "Iteration 31, loss = 0.00693033\n",
      "Iteration 32, loss = 0.00655165\n",
      "Iteration 33, loss = 0.00623161\n",
      "Iteration 34, loss = 0.00594985\n",
      "Iteration 35, loss = 0.00568838\n",
      "Iteration 36, loss = 0.00546816\n",
      "Iteration 37, loss = 0.00541746\n",
      "Iteration 38, loss = 0.00501629\n",
      "Iteration 39, loss = 0.00475161\n",
      "Iteration 40, loss = 0.00460114\n",
      "Iteration 41, loss = 0.00442346\n",
      "Iteration 42, loss = 0.00426412\n",
      "Iteration 43, loss = 0.00403072\n",
      "Iteration 44, loss = 0.00387106\n",
      "Iteration 45, loss = 0.00376004\n",
      "Iteration 46, loss = 0.00356363\n",
      "Iteration 47, loss = 0.00353166\n",
      "Iteration 48, loss = 0.00335730\n",
      "Iteration 49, loss = 0.00325280\n",
      "Iteration 50, loss = 0.00311250\n",
      "Iteration 51, loss = 0.00306385\n",
      "Iteration 52, loss = 0.00302843\n",
      "Iteration 53, loss = 0.00286660\n",
      "Iteration 54, loss = 0.00279436\n",
      "Iteration 55, loss = 0.00263759\n",
      "Iteration 56, loss = 0.00254913\n",
      "Iteration 57, loss = 0.00244024\n",
      "Iteration 58, loss = 0.00240417\n",
      "Iteration 59, loss = 0.00242636\n",
      "Iteration 60, loss = 0.00237248\n",
      "Iteration 61, loss = 0.00225342\n",
      "Iteration 62, loss = 0.00222322\n",
      "Iteration 63, loss = 0.00213241\n",
      "Iteration 64, loss = 0.00234474\n",
      "Iteration 65, loss = 0.00201572\n",
      "Iteration 66, loss = 0.00198965\n",
      "Iteration 67, loss = 0.00191792\n",
      "Iteration 68, loss = 0.00186135\n",
      "Iteration 69, loss = 0.00178088\n",
      "Iteration 70, loss = 0.00180124\n",
      "Iteration 71, loss = 0.00171356\n",
      "Iteration 72, loss = 0.00164174\n",
      "Iteration 73, loss = 0.00166204\n",
      "Iteration 74, loss = 0.00160477\n",
      "Iteration 75, loss = 0.00152608\n",
      "Iteration 76, loss = 0.00149989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54872449\n",
      "Iteration 2, loss = 0.21784955\n",
      "Iteration 3, loss = 0.11852297\n",
      "Iteration 4, loss = 0.08579591\n",
      "Iteration 5, loss = 0.07122252\n",
      "Iteration 6, loss = 0.06148792\n",
      "Iteration 7, loss = 0.05548623\n",
      "Iteration 8, loss = 0.04939792\n",
      "Iteration 9, loss = 0.04520938\n",
      "Iteration 10, loss = 0.04130236\n",
      "Iteration 11, loss = 0.03797181\n",
      "Iteration 12, loss = 0.03505544\n",
      "Iteration 13, loss = 0.03286885\n",
      "Iteration 14, loss = 0.03063134\n",
      "Iteration 15, loss = 0.02871092\n",
      "Iteration 16, loss = 0.02698476\n",
      "Iteration 17, loss = 0.02537958\n",
      "Iteration 18, loss = 0.02398355\n",
      "Iteration 19, loss = 0.02263041\n",
      "Iteration 20, loss = 0.02139759\n",
      "Iteration 21, loss = 0.02039858\n",
      "Iteration 22, loss = 0.01920047\n",
      "Iteration 23, loss = 0.01829004\n",
      "Iteration 24, loss = 0.01742996\n",
      "Iteration 25, loss = 0.01667828\n",
      "Iteration 26, loss = 0.01585975\n",
      "Iteration 27, loss = 0.01523985\n",
      "Iteration 28, loss = 0.01445127\n",
      "Iteration 29, loss = 0.01396423\n",
      "Iteration 30, loss = 0.01336081\n",
      "Iteration 31, loss = 0.01274304\n",
      "Iteration 32, loss = 0.01233506\n",
      "Iteration 33, loss = 0.01181898\n",
      "Iteration 34, loss = 0.01128984\n",
      "Iteration 35, loss = 0.01084669\n",
      "Iteration 36, loss = 0.01041471\n",
      "Iteration 37, loss = 0.01009128\n",
      "Iteration 38, loss = 0.00963562\n",
      "Iteration 39, loss = 0.00931866\n",
      "Iteration 40, loss = 0.00893746\n",
      "Iteration 41, loss = 0.00868802\n",
      "Iteration 42, loss = 0.00830862\n",
      "Iteration 43, loss = 0.00815914\n",
      "Iteration 44, loss = 0.00788868\n",
      "Iteration 45, loss = 0.00764914\n",
      "Iteration 46, loss = 0.00738348\n",
      "Iteration 47, loss = 0.00702892\n",
      "Iteration 48, loss = 0.00689102\n",
      "Iteration 49, loss = 0.00662708\n",
      "Iteration 50, loss = 0.00637047\n",
      "Iteration 51, loss = 0.00616540\n",
      "Iteration 52, loss = 0.00603783\n",
      "Iteration 53, loss = 0.00571604\n",
      "Iteration 54, loss = 0.00560084\n",
      "Iteration 55, loss = 0.00537516\n",
      "Iteration 56, loss = 0.00524384\n",
      "Iteration 57, loss = 0.00509582\n",
      "Iteration 58, loss = 0.00496175\n",
      "Iteration 59, loss = 0.00480693\n",
      "Iteration 60, loss = 0.00473194\n",
      "Iteration 61, loss = 0.00451247\n",
      "Iteration 62, loss = 0.00438235\n",
      "Iteration 63, loss = 0.00431191\n",
      "Iteration 64, loss = 0.00413570\n",
      "Iteration 65, loss = 0.00405018\n",
      "Iteration 66, loss = 0.00394008\n",
      "Iteration 67, loss = 0.00386692\n",
      "Iteration 68, loss = 0.00377038\n",
      "Iteration 69, loss = 0.00365069\n",
      "Iteration 70, loss = 0.00362096\n",
      "Iteration 71, loss = 0.00350115\n",
      "Iteration 72, loss = 0.00344883\n",
      "Iteration 73, loss = 0.00339351\n",
      "Iteration 74, loss = 0.00324878\n",
      "Iteration 75, loss = 0.00318767\n",
      "Iteration 76, loss = 0.00313870\n",
      "Iteration 77, loss = 0.00313901\n",
      "Iteration 78, loss = 0.00299533\n",
      "Iteration 79, loss = 0.00295797\n",
      "Iteration 80, loss = 0.00287587\n",
      "Iteration 81, loss = 0.00292102\n",
      "Iteration 82, loss = 0.00280154\n",
      "Iteration 83, loss = 0.00264401\n",
      "Iteration 84, loss = 0.00272322\n",
      "Iteration 85, loss = 0.00254011\n",
      "Iteration 86, loss = 0.00252772\n",
      "Iteration 87, loss = 0.00243711\n",
      "Iteration 88, loss = 0.00245775\n",
      "Iteration 89, loss = 0.00236297\n",
      "Iteration 90, loss = 0.00227121\n",
      "Iteration 91, loss = 0.00226401\n",
      "Iteration 92, loss = 0.00221701\n",
      "Iteration 93, loss = 0.00216323\n",
      "Iteration 94, loss = 0.00214090\n",
      "Iteration 95, loss = 0.00211445\n",
      "Iteration 96, loss = 0.00204502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48930610\n",
      "Iteration 2, loss = 0.19562899\n",
      "Iteration 3, loss = 0.10954662\n",
      "Iteration 4, loss = 0.07858395\n",
      "Iteration 5, loss = 0.06188491\n",
      "Iteration 6, loss = 0.05200194\n",
      "Iteration 7, loss = 0.04532103\n",
      "Iteration 8, loss = 0.04025745\n",
      "Iteration 9, loss = 0.03620254\n",
      "Iteration 10, loss = 0.03280596\n",
      "Iteration 11, loss = 0.02992521\n",
      "Iteration 12, loss = 0.02760577\n",
      "Iteration 13, loss = 0.02566077\n",
      "Iteration 14, loss = 0.02365001\n",
      "Iteration 15, loss = 0.02200712\n",
      "Iteration 16, loss = 0.02058271\n",
      "Iteration 17, loss = 0.01927657\n",
      "Iteration 18, loss = 0.01828751\n",
      "Iteration 19, loss = 0.01695178\n",
      "Iteration 20, loss = 0.01582991\n",
      "Iteration 21, loss = 0.01511923\n",
      "Iteration 22, loss = 0.01408361\n",
      "Iteration 23, loss = 0.01326226\n",
      "Iteration 24, loss = 0.01252094\n",
      "Iteration 25, loss = 0.01188949\n",
      "Iteration 26, loss = 0.01118119\n",
      "Iteration 27, loss = 0.01073031\n",
      "Iteration 28, loss = 0.01007235\n",
      "Iteration 29, loss = 0.00964810\n",
      "Iteration 30, loss = 0.00913329\n",
      "Iteration 31, loss = 0.00875943\n",
      "Iteration 32, loss = 0.00826729\n",
      "Iteration 33, loss = 0.00790322\n",
      "Iteration 34, loss = 0.00757036\n",
      "Iteration 35, loss = 0.00720840\n",
      "Iteration 36, loss = 0.00692413\n",
      "Iteration 37, loss = 0.00662685\n",
      "Iteration 38, loss = 0.00630167\n",
      "Iteration 39, loss = 0.00605416\n",
      "Iteration 40, loss = 0.00584991\n",
      "Iteration 41, loss = 0.00561034\n",
      "Iteration 42, loss = 0.00542846\n",
      "Iteration 43, loss = 0.00516750\n",
      "Iteration 44, loss = 0.00502049\n",
      "Iteration 45, loss = 0.00479685\n",
      "Iteration 46, loss = 0.00470664\n",
      "Iteration 47, loss = 0.00447919\n",
      "Iteration 48, loss = 0.00443987\n",
      "Iteration 49, loss = 0.00432118\n",
      "Iteration 50, loss = 0.00416534\n",
      "Iteration 51, loss = 0.00399323\n",
      "Iteration 52, loss = 0.00389548\n",
      "Iteration 53, loss = 0.00374333\n",
      "Iteration 54, loss = 0.00367328\n",
      "Iteration 55, loss = 0.00354619\n",
      "Iteration 56, loss = 0.00337508\n",
      "Iteration 57, loss = 0.00342793\n",
      "Iteration 58, loss = 0.00321661\n",
      "Iteration 59, loss = 0.00318921\n",
      "Iteration 60, loss = 0.00311649\n",
      "Iteration 61, loss = 0.00301998\n",
      "Iteration 62, loss = 0.00285772\n",
      "Iteration 63, loss = 0.00290807\n",
      "Iteration 64, loss = 0.00279602\n",
      "Iteration 65, loss = 0.00268348\n",
      "Iteration 66, loss = 0.00261920\n",
      "Iteration 67, loss = 0.00254481\n",
      "Iteration 68, loss = 0.00248329\n",
      "Iteration 69, loss = 0.00250392\n",
      "Iteration 70, loss = 0.00240338\n",
      "Iteration 71, loss = 0.00240169\n",
      "Iteration 72, loss = 0.00230994\n",
      "Iteration 73, loss = 0.00228330\n",
      "Iteration 74, loss = 0.00219078\n",
      "Iteration 75, loss = 0.00213393\n",
      "Iteration 76, loss = 0.00214791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45977115\n",
      "Iteration 2, loss = 0.17104316\n",
      "Iteration 3, loss = 0.09763818\n",
      "Iteration 4, loss = 0.07215881\n",
      "Iteration 5, loss = 0.05784227\n",
      "Iteration 6, loss = 0.04853376\n",
      "Iteration 7, loss = 0.04258651\n",
      "Iteration 8, loss = 0.03782580\n",
      "Iteration 9, loss = 0.03415956\n",
      "Iteration 10, loss = 0.03074313\n",
      "Iteration 11, loss = 0.02819194\n",
      "Iteration 12, loss = 0.02609544\n",
      "Iteration 13, loss = 0.02389479\n",
      "Iteration 14, loss = 0.02213109\n",
      "Iteration 15, loss = 0.02053782\n",
      "Iteration 16, loss = 0.01890086\n",
      "Iteration 17, loss = 0.01745500\n",
      "Iteration 18, loss = 0.01635131\n",
      "Iteration 19, loss = 0.01520277\n",
      "Iteration 20, loss = 0.01429449\n",
      "Iteration 21, loss = 0.01336265\n",
      "Iteration 22, loss = 0.01255348\n",
      "Iteration 23, loss = 0.01184019\n",
      "Iteration 24, loss = 0.01118917\n",
      "Iteration 25, loss = 0.01062893\n",
      "Iteration 26, loss = 0.01004021\n",
      "Iteration 27, loss = 0.00956223\n",
      "Iteration 28, loss = 0.00906267\n",
      "Iteration 29, loss = 0.00859669\n",
      "Iteration 30, loss = 0.00813919\n",
      "Iteration 31, loss = 0.00775648\n",
      "Iteration 32, loss = 0.00746228\n",
      "Iteration 33, loss = 0.00709380\n",
      "Iteration 34, loss = 0.00676686\n",
      "Iteration 35, loss = 0.00648055\n",
      "Iteration 36, loss = 0.00622033\n",
      "Iteration 37, loss = 0.00599054\n",
      "Iteration 38, loss = 0.00575999\n",
      "Iteration 39, loss = 0.00550914\n",
      "Iteration 40, loss = 0.00532651\n",
      "Iteration 41, loss = 0.00515779\n",
      "Iteration 42, loss = 0.00492012\n",
      "Iteration 43, loss = 0.00478282\n",
      "Iteration 44, loss = 0.00461183\n",
      "Iteration 45, loss = 0.00448653\n",
      "Iteration 46, loss = 0.00429943\n",
      "Iteration 47, loss = 0.00416601\n",
      "Iteration 48, loss = 0.00406041\n",
      "Iteration 49, loss = 0.00391374\n",
      "Iteration 50, loss = 0.00382518\n",
      "Iteration 51, loss = 0.00370122\n",
      "Iteration 52, loss = 0.00367743\n",
      "Iteration 53, loss = 0.00349526\n",
      "Iteration 54, loss = 0.00342510\n",
      "Iteration 55, loss = 0.00330514\n",
      "Iteration 56, loss = 0.00318929\n",
      "Iteration 57, loss = 0.00310691\n",
      "Iteration 58, loss = 0.00299100\n",
      "Iteration 59, loss = 0.00299592\n",
      "Iteration 60, loss = 0.00288962\n",
      "Iteration 61, loss = 0.00277506\n",
      "Iteration 62, loss = 0.00268061\n",
      "Iteration 63, loss = 0.00271344\n",
      "Iteration 64, loss = 0.00266511\n",
      "Iteration 65, loss = 0.00250295\n",
      "Iteration 66, loss = 0.00245992\n",
      "Iteration 67, loss = 0.00239880\n",
      "Iteration 68, loss = 0.00256042\n",
      "Iteration 69, loss = 0.00233107\n",
      "Iteration 70, loss = 0.00227979\n",
      "Iteration 71, loss = 0.00218453\n",
      "Iteration 72, loss = 0.00214053\n",
      "Iteration 73, loss = 0.00213130\n",
      "Iteration 74, loss = 0.00207738\n",
      "Iteration 75, loss = 0.00198962\n",
      "Iteration 76, loss = 0.00194254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03069588\n",
      "Iteration 2, loss = 0.41813269\n",
      "Iteration 3, loss = 0.20403135\n",
      "Iteration 4, loss = 0.12862960\n",
      "Iteration 5, loss = 0.09705603\n",
      "Iteration 6, loss = 0.07950836\n",
      "Iteration 7, loss = 0.06919767\n",
      "Iteration 8, loss = 0.06142660\n",
      "Iteration 9, loss = 0.05569009\n",
      "Iteration 10, loss = 0.05097137\n",
      "Iteration 11, loss = 0.04742196\n",
      "Iteration 12, loss = 0.04426573\n",
      "Iteration 13, loss = 0.04142968\n",
      "Iteration 14, loss = 0.03891425\n",
      "Iteration 15, loss = 0.03663164\n",
      "Iteration 16, loss = 0.03471395\n",
      "Iteration 17, loss = 0.03274709\n",
      "Iteration 18, loss = 0.03104781\n",
      "Iteration 19, loss = 0.02960302\n",
      "Iteration 20, loss = 0.02820232\n",
      "Iteration 21, loss = 0.02690017\n",
      "Iteration 22, loss = 0.02557974\n",
      "Iteration 23, loss = 0.02441714\n",
      "Iteration 24, loss = 0.02356738\n",
      "Iteration 25, loss = 0.02239125\n",
      "Iteration 26, loss = 0.02135282\n",
      "Iteration 27, loss = 0.02052232\n",
      "Iteration 28, loss = 0.01973304\n",
      "Iteration 29, loss = 0.01890767\n",
      "Iteration 30, loss = 0.01817849\n",
      "Iteration 31, loss = 0.01741523\n",
      "Iteration 32, loss = 0.01677195\n",
      "Iteration 33, loss = 0.01612094\n",
      "Iteration 34, loss = 0.01552826\n",
      "Iteration 35, loss = 0.01503390\n",
      "Iteration 36, loss = 0.01448257\n",
      "Iteration 37, loss = 0.01402980\n",
      "Iteration 38, loss = 0.01356003\n",
      "Iteration 39, loss = 0.01310266\n",
      "Iteration 40, loss = 0.01268350\n",
      "Iteration 41, loss = 0.01218463\n",
      "Iteration 42, loss = 0.01188781\n",
      "Iteration 43, loss = 0.01148622\n",
      "Iteration 44, loss = 0.01113581\n",
      "Iteration 45, loss = 0.01078002\n",
      "Iteration 46, loss = 0.01039780\n",
      "Iteration 47, loss = 0.01013313\n",
      "Iteration 48, loss = 0.00979756\n",
      "Iteration 49, loss = 0.00954982\n",
      "Iteration 50, loss = 0.00930768\n",
      "Iteration 51, loss = 0.00893548\n",
      "Iteration 52, loss = 0.00870638\n",
      "Iteration 53, loss = 0.00853460\n",
      "Iteration 54, loss = 0.00826758\n",
      "Iteration 55, loss = 0.00798668\n",
      "Iteration 56, loss = 0.00773993\n",
      "Iteration 57, loss = 0.00759917\n",
      "Iteration 58, loss = 0.00734053\n",
      "Iteration 59, loss = 0.00713709\n",
      "Iteration 60, loss = 0.00706973\n",
      "Iteration 61, loss = 0.00680240\n",
      "Iteration 62, loss = 0.00660519\n",
      "Iteration 63, loss = 0.00641518\n",
      "Iteration 64, loss = 0.00621573\n",
      "Iteration 65, loss = 0.00606016\n",
      "Iteration 66, loss = 0.00592560\n",
      "Iteration 67, loss = 0.00578574\n",
      "Iteration 68, loss = 0.00564059\n",
      "Iteration 69, loss = 0.00550395\n",
      "Iteration 70, loss = 0.00536041\n",
      "Iteration 71, loss = 0.00522731\n",
      "Iteration 72, loss = 0.00508763\n",
      "Iteration 73, loss = 0.00502363\n",
      "Iteration 74, loss = 0.00486804\n",
      "Iteration 75, loss = 0.00481857\n",
      "Iteration 76, loss = 0.00467489\n",
      "Iteration 77, loss = 0.00452801\n",
      "Iteration 78, loss = 0.00445749\n",
      "Iteration 79, loss = 0.00437326\n",
      "Iteration 80, loss = 0.00424298\n",
      "Iteration 81, loss = 0.00415430\n",
      "Iteration 82, loss = 0.00404093\n",
      "Iteration 83, loss = 0.00400853\n",
      "Iteration 84, loss = 0.00389085\n",
      "Iteration 85, loss = 0.00381049\n",
      "Iteration 86, loss = 0.00373980\n",
      "Iteration 87, loss = 0.00371359\n",
      "Iteration 88, loss = 0.00357795\n",
      "Iteration 89, loss = 0.00352694\n",
      "Iteration 90, loss = 0.00343490\n",
      "Iteration 91, loss = 0.00340204\n",
      "Iteration 92, loss = 0.00328973\n",
      "Iteration 93, loss = 0.00327526\n",
      "Iteration 94, loss = 0.00326250\n",
      "Iteration 95, loss = 0.00312383\n",
      "Iteration 96, loss = 0.00304254\n",
      "Iteration 97, loss = 0.00302187\n",
      "Iteration 98, loss = 0.00300009\n",
      "Iteration 99, loss = 0.00292819\n",
      "Iteration 100, loss = 0.00285076\n",
      "Iteration 101, loss = 0.00278462\n",
      "Iteration 102, loss = 0.00272982\n",
      "Iteration 103, loss = 0.00271467\n",
      "Iteration 104, loss = 0.00269884\n",
      "Iteration 105, loss = 0.00260553\n",
      "Iteration 106, loss = 0.00259592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75019124\n",
      "Iteration 2, loss = 0.29501639\n",
      "Iteration 3, loss = 0.14925370\n",
      "Iteration 4, loss = 0.09844697\n",
      "Iteration 5, loss = 0.07665545\n",
      "Iteration 6, loss = 0.06483944\n",
      "Iteration 7, loss = 0.05695389\n",
      "Iteration 8, loss = 0.05088167\n",
      "Iteration 9, loss = 0.04776715\n",
      "Iteration 10, loss = 0.04436454\n",
      "Iteration 11, loss = 0.04028853\n",
      "Iteration 12, loss = 0.03793176\n",
      "Iteration 13, loss = 0.03527181\n",
      "Iteration 14, loss = 0.03311050\n",
      "Iteration 15, loss = 0.03110108\n",
      "Iteration 16, loss = 0.02951058\n",
      "Iteration 17, loss = 0.02807259\n",
      "Iteration 18, loss = 0.02701886\n",
      "Iteration 19, loss = 0.02548482\n",
      "Iteration 20, loss = 0.02422636\n",
      "Iteration 21, loss = 0.02297295\n",
      "Iteration 22, loss = 0.02183346\n",
      "Iteration 23, loss = 0.02083291\n",
      "Iteration 24, loss = 0.02023340\n",
      "Iteration 25, loss = 0.01952731\n",
      "Iteration 26, loss = 0.01883481\n",
      "Iteration 27, loss = 0.01790885\n",
      "Iteration 28, loss = 0.01704436\n",
      "Iteration 29, loss = 0.01633927\n",
      "Iteration 30, loss = 0.01571788\n",
      "Iteration 31, loss = 0.01518958\n",
      "Iteration 32, loss = 0.01465550\n",
      "Iteration 33, loss = 0.01408771\n",
      "Iteration 34, loss = 0.01364262\n",
      "Iteration 35, loss = 0.01349260\n",
      "Iteration 36, loss = 0.01296101\n",
      "Iteration 37, loss = 0.01246875\n",
      "Iteration 38, loss = 0.01204567\n",
      "Iteration 39, loss = 0.01165246\n",
      "Iteration 40, loss = 0.01129583\n",
      "Iteration 41, loss = 0.01095166\n",
      "Iteration 42, loss = 0.01060941\n",
      "Iteration 43, loss = 0.01030404\n",
      "Iteration 44, loss = 0.01000101\n",
      "Iteration 45, loss = 0.00977621\n",
      "Iteration 46, loss = 0.00969993\n",
      "Iteration 47, loss = 0.00931808\n",
      "Iteration 48, loss = 0.00900767\n",
      "Iteration 49, loss = 0.00869260\n",
      "Iteration 50, loss = 0.00838427\n",
      "Iteration 51, loss = 0.00816101\n",
      "Iteration 52, loss = 0.00793514\n",
      "Iteration 53, loss = 0.00767087\n",
      "Iteration 54, loss = 0.00741544\n",
      "Iteration 55, loss = 0.00747104\n",
      "Iteration 56, loss = 0.00727087\n",
      "Iteration 57, loss = 0.00689601\n",
      "Iteration 58, loss = 0.00660684\n",
      "Iteration 59, loss = 0.00640459\n",
      "Iteration 60, loss = 0.00622441\n",
      "Iteration 61, loss = 0.00603185\n",
      "Iteration 62, loss = 0.00589033\n",
      "Iteration 63, loss = 0.00583974\n",
      "Iteration 64, loss = 0.00657018\n",
      "Iteration 65, loss = 0.00610160\n",
      "Iteration 66, loss = 0.00550325\n",
      "Iteration 67, loss = 0.00514716\n",
      "Iteration 68, loss = 0.00493751\n",
      "Iteration 69, loss = 0.00474934\n",
      "Iteration 70, loss = 0.00459968\n",
      "Iteration 71, loss = 0.00443282\n",
      "Iteration 72, loss = 0.00431441\n",
      "Iteration 73, loss = 0.00417659\n",
      "Iteration 74, loss = 0.00410595\n",
      "Iteration 75, loss = 0.00405823\n",
      "Iteration 76, loss = 0.00388516\n",
      "Iteration 77, loss = 0.00376612\n",
      "Iteration 78, loss = 0.00370163\n",
      "Iteration 79, loss = 0.00361582\n",
      "Iteration 80, loss = 0.00350224\n",
      "Iteration 81, loss = 0.00338318\n",
      "Iteration 82, loss = 0.00328963\n",
      "Iteration 83, loss = 0.00320996\n",
      "Iteration 84, loss = 0.00314096\n",
      "Iteration 85, loss = 0.00307808\n",
      "Iteration 86, loss = 0.00299642\n",
      "Iteration 87, loss = 0.00294105\n",
      "Iteration 88, loss = 0.00286854\n",
      "Iteration 89, loss = 0.00282481\n",
      "Iteration 90, loss = 0.00277943\n",
      "Iteration 91, loss = 0.00270051\n",
      "Iteration 92, loss = 0.00266979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56824386\n",
      "Iteration 2, loss = 0.22074949\n",
      "Iteration 3, loss = 0.12548823\n",
      "Iteration 4, loss = 0.09053198\n",
      "Iteration 5, loss = 0.07222677\n",
      "Iteration 6, loss = 0.06049131\n",
      "Iteration 7, loss = 0.05329672\n",
      "Iteration 8, loss = 0.04756090\n",
      "Iteration 9, loss = 0.04336724\n",
      "Iteration 10, loss = 0.04017878\n",
      "Iteration 11, loss = 0.03687382\n",
      "Iteration 12, loss = 0.03471295\n",
      "Iteration 13, loss = 0.03262167\n",
      "Iteration 14, loss = 0.03033959\n",
      "Iteration 15, loss = 0.02871219\n",
      "Iteration 16, loss = 0.02767092\n",
      "Iteration 17, loss = 0.02624649\n",
      "Iteration 18, loss = 0.02568324\n",
      "Iteration 19, loss = 0.02532022\n",
      "Iteration 20, loss = 0.02395747\n",
      "Iteration 21, loss = 0.02323515\n",
      "Iteration 22, loss = 0.02197923\n",
      "Iteration 23, loss = 0.02052503\n",
      "Iteration 24, loss = 0.01954658\n",
      "Iteration 25, loss = 0.01885404\n",
      "Iteration 26, loss = 0.01805386\n",
      "Iteration 27, loss = 0.01732787\n",
      "Iteration 28, loss = 0.01823536\n",
      "Iteration 29, loss = 0.01847741\n",
      "Iteration 30, loss = 0.01584774\n",
      "Iteration 31, loss = 0.01549582\n",
      "Iteration 32, loss = 0.01469246\n",
      "Iteration 33, loss = 0.01408564\n",
      "Iteration 34, loss = 0.01336364\n",
      "Iteration 35, loss = 0.01289934\n",
      "Iteration 36, loss = 0.01248245\n",
      "Iteration 37, loss = 0.01202613\n",
      "Iteration 38, loss = 0.01166186\n",
      "Iteration 39, loss = 0.01129521\n",
      "Iteration 40, loss = 0.01092827\n",
      "Iteration 41, loss = 0.01062929\n",
      "Iteration 42, loss = 0.01034912\n",
      "Iteration 43, loss = 0.00997885\n",
      "Iteration 44, loss = 0.00992811\n",
      "Iteration 45, loss = 0.00964488\n",
      "Iteration 46, loss = 0.00943720\n",
      "Iteration 47, loss = 0.00908626\n",
      "Iteration 48, loss = 0.00871883\n",
      "Iteration 49, loss = 0.00892761\n",
      "Iteration 50, loss = 0.00904596\n",
      "Iteration 51, loss = 0.01089667\n",
      "Iteration 52, loss = 0.00979296\n",
      "Iteration 53, loss = 0.00841175\n",
      "Iteration 54, loss = 0.00766695\n",
      "Iteration 55, loss = 0.00723643\n",
      "Iteration 56, loss = 0.00692833\n",
      "Iteration 57, loss = 0.00667269\n",
      "Iteration 58, loss = 0.00641435\n",
      "Iteration 59, loss = 0.00623887\n",
      "Iteration 60, loss = 0.00603184\n",
      "Iteration 61, loss = 0.00589054\n",
      "Iteration 62, loss = 0.00567923\n",
      "Iteration 63, loss = 0.00547376\n",
      "Iteration 64, loss = 0.00530873\n",
      "Iteration 65, loss = 0.00517411\n",
      "Iteration 66, loss = 0.00502758\n",
      "Iteration 67, loss = 0.00489861\n",
      "Iteration 68, loss = 0.00478515\n",
      "Iteration 69, loss = 0.00463805\n",
      "Iteration 70, loss = 0.00448620\n",
      "Iteration 71, loss = 0.00435222\n",
      "Iteration 72, loss = 0.00424871\n",
      "Iteration 73, loss = 0.00413875\n",
      "Iteration 74, loss = 0.00402620\n",
      "Iteration 75, loss = 0.00392817\n",
      "Iteration 76, loss = 0.00393881\n",
      "Iteration 77, loss = 0.00404749\n",
      "Iteration 78, loss = 0.00383948\n",
      "Iteration 79, loss = 0.00365728\n",
      "Iteration 80, loss = 0.00348580\n",
      "Iteration 81, loss = 0.00337362\n",
      "Iteration 82, loss = 0.00331096\n",
      "Iteration 83, loss = 0.00321432\n",
      "Iteration 84, loss = 0.00313740\n",
      "Iteration 85, loss = 0.00305554\n",
      "Iteration 86, loss = 0.00298288\n",
      "Iteration 87, loss = 0.00292396\n",
      "Iteration 88, loss = 0.00285601\n",
      "Iteration 89, loss = 0.00278692\n",
      "Iteration 90, loss = 0.00271846\n",
      "Iteration 91, loss = 0.00265972\n",
      "Iteration 92, loss = 0.00260723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69284580\n",
      "Iteration 2, loss = 0.25522998\n",
      "Iteration 3, loss = 0.13585320\n",
      "Iteration 4, loss = 0.09557417\n",
      "Iteration 5, loss = 0.07652842\n",
      "Iteration 6, loss = 0.06480994\n",
      "Iteration 7, loss = 0.05605693\n",
      "Iteration 8, loss = 0.05024842\n",
      "Iteration 9, loss = 0.04537373\n",
      "Iteration 10, loss = 0.04191441\n",
      "Iteration 11, loss = 0.03850446\n",
      "Iteration 12, loss = 0.03587286\n",
      "Iteration 13, loss = 0.03363034\n",
      "Iteration 14, loss = 0.03161977\n",
      "Iteration 15, loss = 0.02960166\n",
      "Iteration 16, loss = 0.02802142\n",
      "Iteration 17, loss = 0.02664488\n",
      "Iteration 18, loss = 0.02521712\n",
      "Iteration 19, loss = 0.02388352\n",
      "Iteration 20, loss = 0.02284647\n",
      "Iteration 21, loss = 0.02186687\n",
      "Iteration 22, loss = 0.02072456\n",
      "Iteration 23, loss = 0.01978896\n",
      "Iteration 24, loss = 0.01904379\n",
      "Iteration 25, loss = 0.01824851\n",
      "Iteration 26, loss = 0.01743906\n",
      "Iteration 27, loss = 0.01675451\n",
      "Iteration 28, loss = 0.01606263\n",
      "Iteration 29, loss = 0.01558854\n",
      "Iteration 30, loss = 0.01479036\n",
      "Iteration 31, loss = 0.01434411\n",
      "Iteration 32, loss = 0.01376841\n",
      "Iteration 33, loss = 0.01323215\n",
      "Iteration 34, loss = 0.01270092\n",
      "Iteration 35, loss = 0.01224092\n",
      "Iteration 36, loss = 0.01185924\n",
      "Iteration 37, loss = 0.01145984\n",
      "Iteration 38, loss = 0.01102789\n",
      "Iteration 39, loss = 0.01076820\n",
      "Iteration 40, loss = 0.01032827\n",
      "Iteration 41, loss = 0.01017618\n",
      "Iteration 42, loss = 0.00972680\n",
      "Iteration 43, loss = 0.00936269\n",
      "Iteration 44, loss = 0.00911646\n",
      "Iteration 45, loss = 0.00883976\n",
      "Iteration 46, loss = 0.00855372\n",
      "Iteration 47, loss = 0.00822681\n",
      "Iteration 48, loss = 0.00796920\n",
      "Iteration 49, loss = 0.00769157\n",
      "Iteration 50, loss = 0.00746676\n",
      "Iteration 51, loss = 0.00727526\n",
      "Iteration 52, loss = 0.00706664\n",
      "Iteration 53, loss = 0.00687609\n",
      "Iteration 54, loss = 0.00669828\n",
      "Iteration 55, loss = 0.00659285\n",
      "Iteration 56, loss = 0.00630103\n",
      "Iteration 57, loss = 0.00614020\n",
      "Iteration 58, loss = 0.00585356\n",
      "Iteration 59, loss = 0.00578873\n",
      "Iteration 60, loss = 0.00553828\n",
      "Iteration 61, loss = 0.00555317\n",
      "Iteration 62, loss = 0.00530725\n",
      "Iteration 63, loss = 0.00522180\n",
      "Iteration 64, loss = 0.00505518\n",
      "Iteration 65, loss = 0.00483434\n",
      "Iteration 66, loss = 0.00473189\n",
      "Iteration 67, loss = 0.00458116\n",
      "Iteration 68, loss = 0.00448003\n",
      "Iteration 69, loss = 0.00444162\n",
      "Iteration 70, loss = 0.00430291\n",
      "Iteration 71, loss = 0.00412716\n",
      "Iteration 72, loss = 0.00403306\n",
      "Iteration 73, loss = 0.00396485\n",
      "Iteration 74, loss = 0.00384639\n",
      "Iteration 75, loss = 0.00378801\n",
      "Iteration 76, loss = 0.00363099\n",
      "Iteration 77, loss = 0.00372647\n",
      "Iteration 78, loss = 0.00367055\n",
      "Iteration 79, loss = 0.00338560\n",
      "Iteration 80, loss = 0.00349448\n",
      "Iteration 81, loss = 0.00333743\n",
      "Iteration 82, loss = 0.00320896\n",
      "Iteration 83, loss = 0.00317804\n",
      "Iteration 84, loss = 0.00315690\n",
      "Iteration 85, loss = 0.00300385\n",
      "Iteration 86, loss = 0.00322651\n",
      "Iteration 87, loss = 0.00294894\n",
      "Iteration 88, loss = 0.00280819\n",
      "Iteration 89, loss = 0.00282979\n",
      "Iteration 90, loss = 0.00273531\n",
      "Iteration 91, loss = 0.00269137\n",
      "Iteration 92, loss = 0.00259065\n",
      "Iteration 93, loss = 0.00259099\n",
      "Iteration 94, loss = 0.00259966\n",
      "Iteration 95, loss = 0.00247224\n",
      "Iteration 96, loss = 0.00246256\n",
      "Iteration 97, loss = 0.00238566\n",
      "Iteration 98, loss = 0.00236621\n",
      "Iteration 99, loss = 0.00228507\n",
      "Iteration 100, loss = 0.00226940\n",
      "Iteration 101, loss = 0.00222536\n",
      "Iteration 102, loss = 0.00214634\n",
      "Iteration 103, loss = 0.00212856\n",
      "Iteration 104, loss = 0.00224311\n",
      "Iteration 105, loss = 0.00209862\n",
      "Iteration 106, loss = 0.00204493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72852789\n",
      "Iteration 2, loss = 0.28532950\n",
      "Iteration 3, loss = 0.15169547\n",
      "Iteration 4, loss = 0.10386415\n",
      "Iteration 5, loss = 0.08186098\n",
      "Iteration 6, loss = 0.06872705\n",
      "Iteration 7, loss = 0.06003379\n",
      "Iteration 8, loss = 0.05354927\n",
      "Iteration 9, loss = 0.04861271\n",
      "Iteration 10, loss = 0.04426671\n",
      "Iteration 11, loss = 0.04108921\n",
      "Iteration 12, loss = 0.03791999\n",
      "Iteration 13, loss = 0.03524084\n",
      "Iteration 14, loss = 0.03285420\n",
      "Iteration 15, loss = 0.03074156\n",
      "Iteration 16, loss = 0.02869646\n",
      "Iteration 17, loss = 0.02710112\n",
      "Iteration 18, loss = 0.02569418\n",
      "Iteration 19, loss = 0.02422356\n",
      "Iteration 20, loss = 0.02315526\n",
      "Iteration 21, loss = 0.02192954\n",
      "Iteration 22, loss = 0.02060416\n",
      "Iteration 23, loss = 0.01968046\n",
      "Iteration 24, loss = 0.01897175\n",
      "Iteration 25, loss = 0.01795436\n",
      "Iteration 26, loss = 0.01716098\n",
      "Iteration 27, loss = 0.01635640\n",
      "Iteration 28, loss = 0.01572294\n",
      "Iteration 29, loss = 0.01506886\n",
      "Iteration 30, loss = 0.01448084\n",
      "Iteration 31, loss = 0.01386486\n",
      "Iteration 32, loss = 0.01339015\n",
      "Iteration 33, loss = 0.01282079\n",
      "Iteration 34, loss = 0.01233949\n",
      "Iteration 35, loss = 0.01190990\n",
      "Iteration 36, loss = 0.01147708\n",
      "Iteration 37, loss = 0.01102986\n",
      "Iteration 38, loss = 0.01068852\n",
      "Iteration 39, loss = 0.01027668\n",
      "Iteration 40, loss = 0.00994471\n",
      "Iteration 41, loss = 0.00959866\n",
      "Iteration 42, loss = 0.00926842\n",
      "Iteration 43, loss = 0.00890424\n",
      "Iteration 44, loss = 0.00865191\n",
      "Iteration 45, loss = 0.00841465\n",
      "Iteration 46, loss = 0.00803714\n",
      "Iteration 47, loss = 0.00790457\n",
      "Iteration 48, loss = 0.00751395\n",
      "Iteration 49, loss = 0.00719970\n",
      "Iteration 50, loss = 0.00706247\n",
      "Iteration 51, loss = 0.00682199\n",
      "Iteration 52, loss = 0.00655687\n",
      "Iteration 53, loss = 0.00640023\n",
      "Iteration 54, loss = 0.00622564\n",
      "Iteration 55, loss = 0.00607411\n",
      "Iteration 56, loss = 0.00578134\n",
      "Iteration 57, loss = 0.00558934\n",
      "Iteration 58, loss = 0.00547929\n",
      "Iteration 59, loss = 0.00537734\n",
      "Iteration 60, loss = 0.00518154\n",
      "Iteration 61, loss = 0.00497940\n",
      "Iteration 62, loss = 0.00485958\n",
      "Iteration 63, loss = 0.00470490\n",
      "Iteration 64, loss = 0.00458340\n",
      "Iteration 65, loss = 0.00450771\n",
      "Iteration 66, loss = 0.00436377\n",
      "Iteration 67, loss = 0.00428246\n",
      "Iteration 68, loss = 0.00415023\n",
      "Iteration 69, loss = 0.00410033\n",
      "Iteration 70, loss = 0.00398716\n",
      "Iteration 71, loss = 0.00391067\n",
      "Iteration 72, loss = 0.00377600\n",
      "Iteration 73, loss = 0.00366007\n",
      "Iteration 74, loss = 0.00361633\n",
      "Iteration 75, loss = 0.00354480\n",
      "Iteration 76, loss = 0.00343032\n",
      "Iteration 77, loss = 0.00334450\n",
      "Iteration 78, loss = 0.00325486\n",
      "Iteration 79, loss = 0.00322768\n",
      "Iteration 80, loss = 0.00328467\n",
      "Iteration 81, loss = 0.00319674\n",
      "Iteration 82, loss = 0.00301166\n",
      "Iteration 83, loss = 0.00293368\n",
      "Iteration 84, loss = 0.00302321\n",
      "Iteration 85, loss = 0.00278563\n",
      "Iteration 86, loss = 0.00279904\n",
      "Iteration 87, loss = 0.00277365\n",
      "Iteration 88, loss = 0.00273511\n",
      "Iteration 89, loss = 0.00265941\n",
      "Iteration 90, loss = 0.00255352\n",
      "Iteration 91, loss = 0.00250025\n",
      "Iteration 92, loss = 0.00245897\n",
      "Iteration 93, loss = 0.00241049\n",
      "Iteration 94, loss = 0.00239889\n",
      "Iteration 95, loss = 0.00238631\n",
      "Iteration 96, loss = 0.00231658\n",
      "Iteration 97, loss = 0.00224229\n",
      "Iteration 98, loss = 0.00219813\n",
      "Iteration 99, loss = 0.00220506\n",
      "Iteration 100, loss = 0.00211626\n",
      "Iteration 101, loss = 0.00210545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37319048\n",
      "Iteration 2, loss = 0.15389872\n",
      "Iteration 3, loss = 0.08987093\n",
      "Iteration 4, loss = 0.06484483\n",
      "Iteration 5, loss = 0.05296258\n",
      "Iteration 6, loss = 0.04481514\n",
      "Iteration 7, loss = 0.03942413\n",
      "Iteration 8, loss = 0.03555746\n",
      "Iteration 9, loss = 0.03218569\n",
      "Iteration 10, loss = 0.02986297\n",
      "Iteration 11, loss = 0.02756318\n",
      "Iteration 12, loss = 0.02551016\n",
      "Iteration 13, loss = 0.02388876\n",
      "Iteration 14, loss = 0.02213492\n",
      "Iteration 15, loss = 0.02068513\n",
      "Iteration 16, loss = 0.01934399\n",
      "Iteration 17, loss = 0.01833506\n",
      "Iteration 18, loss = 0.01733977\n",
      "Iteration 19, loss = 0.01623785\n",
      "Iteration 20, loss = 0.01544681\n",
      "Iteration 21, loss = 0.01456633\n",
      "Iteration 22, loss = 0.01382447\n",
      "Iteration 23, loss = 0.01314448\n",
      "Iteration 24, loss = 0.01249552\n",
      "Iteration 25, loss = 0.01186446\n",
      "Iteration 26, loss = 0.01137153\n",
      "Iteration 27, loss = 0.01077703\n",
      "Iteration 28, loss = 0.01036630\n",
      "Iteration 29, loss = 0.00988421\n",
      "Iteration 30, loss = 0.00937412\n",
      "Iteration 31, loss = 0.00902879\n",
      "Iteration 32, loss = 0.00863615\n",
      "Iteration 33, loss = 0.00823944\n",
      "Iteration 34, loss = 0.00796637\n",
      "Iteration 35, loss = 0.00757103\n",
      "Iteration 36, loss = 0.00743101\n",
      "Iteration 37, loss = 0.00702266\n",
      "Iteration 38, loss = 0.00670841\n",
      "Iteration 39, loss = 0.00651159\n",
      "Iteration 40, loss = 0.00625693\n",
      "Iteration 41, loss = 0.00604241\n",
      "Iteration 42, loss = 0.00585786\n",
      "Iteration 43, loss = 0.00557944\n",
      "Iteration 44, loss = 0.00542892\n",
      "Iteration 45, loss = 0.00528123\n",
      "Iteration 46, loss = 0.00501513\n",
      "Iteration 47, loss = 0.00494662\n",
      "Iteration 48, loss = 0.00477074\n",
      "Iteration 49, loss = 0.00468167\n",
      "Iteration 50, loss = 0.00439934\n",
      "Iteration 51, loss = 0.00448520\n",
      "Iteration 52, loss = 0.00432422\n",
      "Iteration 53, loss = 0.00398869\n",
      "Iteration 54, loss = 0.00392532\n",
      "Iteration 55, loss = 0.00375449\n",
      "Iteration 56, loss = 0.00377201\n",
      "Iteration 57, loss = 0.00363781\n",
      "Iteration 58, loss = 0.00346922\n",
      "Iteration 59, loss = 0.00334971\n",
      "Iteration 60, loss = 0.00323007\n",
      "Iteration 61, loss = 0.00323407\n",
      "Iteration 62, loss = 0.00307539\n",
      "Iteration 63, loss = 0.00314856\n",
      "Iteration 64, loss = 0.00294850\n",
      "Iteration 65, loss = 0.00286316\n",
      "Iteration 66, loss = 0.00280236\n",
      "Iteration 67, loss = 0.00277981\n",
      "Iteration 68, loss = 0.00262153\n",
      "Iteration 69, loss = 0.00264188\n",
      "Iteration 70, loss = 0.00257368\n",
      "Iteration 71, loss = 0.00254676\n",
      "Iteration 72, loss = 0.00248911\n",
      "Iteration 73, loss = 0.00239527\n",
      "Iteration 74, loss = 0.00233011\n",
      "Iteration 75, loss = 0.00226307\n",
      "Iteration 76, loss = 0.00221005\n",
      "Iteration 77, loss = 0.00218925\n",
      "Iteration 78, loss = 0.00210266\n",
      "Iteration 79, loss = 0.00212650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "test_roc1=[]\n",
    "test_roc12=[]\n",
    "test_prc1=[]\n",
    "test_prc12=[]\n",
    "def get_fps(mol):\n",
    "    calc=MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])\n",
    "    ds = np.asarray(calc.CalcDescriptors(mol))\n",
    "    arr=Fingerprinter.FingerprintMol(mol)[0]\n",
    "    \n",
    "    return np.append(arr,ds)\n",
    "#全部的特征随即森林\n",
    "for i in range(10):\n",
    "    data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "    data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    data['Descriptors']=data['mol'].apply(get_fps)\n",
    "    data['Descriptors'].fillna(data.mean())\n",
    "    xtrain = np.array(list(data['Descriptors']))\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    xtrain[xtrain >= np.finfo(np.float32).max]=np.finfo(np.float32).max\n",
    "    ytrain= np.array(data['activity'])\n",
    "   \n",
    "    test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "    test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    test['Descriptors']=test['mol'].apply(get_fps)\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    \n",
    "    xtest = np.array(list(test['Descriptors']))\n",
    "    xtest=np.nan_to_num(xtest)\n",
    "    xtest[xtest >= np.finfo(np.float32).max] = np.finfo(np.float32).max\n",
    "    ytest=test['activity']\n",
    "    xtrain = StandardScaler().fit_transform(xtrain)\n",
    "    xtest = StandardScaler().fit_transform(xtest)\n",
    "    rf = RandomForestClassifier(max_features='auto')\n",
    "    rf2=MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=10)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    rf2.fit(xtrain, ytrain)\n",
    "    predict_prob_y = rf.predict_proba(xtest)\n",
    "    predict_prob_y2 = rf2.predict_proba(xtest)\n",
    "    test_roc1.append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "    test_roc12.append(metrics.roc_auc_score(ytest,predict_prob_y2[:,1]))\n",
    "    fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "    fpr2, tpr2, thresholds=(precision_recall_curve(ytest,predict_prob_y2[:,1]))\n",
    "    test_prc1.append(metrics.auc( tpr,fpr))\n",
    "    test_prc12.append(metrics.auc( tpr2,fpr2))\n",
    "    del rf\n",
    "    del rf2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从打印结果中可以看到，分子描述符特征的随机森林roc，prc结果为0.81和0.45，mlp方法结果为0.79和0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811045867532132\n",
      "0.4549249079343823\n",
      "0.7930097884300603\n",
      "0.4413498076293891\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_roc1))\n",
    "print(np.mean(test_prc1))\n",
    "print(np.mean(test_roc12))\n",
    "print(np.mean(test_prc12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印具体每一折的roc和prc,可以看到即使是在同一特征上两种方法在不同折的表现也不尽相同，可以参照第一折即可看到两种方法的不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7204081632653061, 0.9295212765957447, 0.8946700507614214, 0.682449494949495, 0.9166666666666666, 0.9048821548821548, 0.7974452554744526, 0.45169082125603865, 0.9911167512690356, 0.821608040201005]\n",
      "[0.42172121040550326, 0.6071080976250148, 0.7607418971156705, 0.24633341285661292, 0.8423054699537751, 0.5420005341880342, 0.15884387351778653, 0.0037593984962406013, 0.8333333333333334, 0.13310185185185183]\n",
      "[0.95, 0.9293313069908815, 0.8375634517766497, 0.6590909090909091, 0.9188034188034189, 0.6616161616161615, 0.738138686131387, 0.5458937198067633, 0.9961928934010151, 0.6934673366834171]\n",
      "[0.6021910929033963, 0.904919169540045, 0.5301480156148342, 0.2736304147778606, 0.7549646686159845, 0.40302682371210286, 0.025902701140041076, 0.005263157894736842, 0.8839285714285714, 0.029523460666317808]\n"
     ]
    }
   ],
   "source": [
    "print(test_roc1)\n",
    "print(test_prc1)\n",
    "print(test_roc12)\n",
    "print(test_prc12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二个实验 mol2vec与所有描述符的混合经过随机森林和mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.16041792\n",
      "Iteration 2, loss = 0.06183708\n",
      "Iteration 3, loss = 0.04667468\n",
      "Iteration 4, loss = 0.03592574\n",
      "Iteration 5, loss = 0.02878874\n",
      "Iteration 6, loss = 0.02385688\n",
      "Iteration 7, loss = 0.01938355\n",
      "Iteration 8, loss = 0.01706065\n",
      "Iteration 9, loss = 0.01479958\n",
      "Iteration 10, loss = 0.01265891\n",
      "Iteration 11, loss = 0.01142854\n",
      "Iteration 12, loss = 0.01001006\n",
      "Iteration 13, loss = 0.00906774\n",
      "Iteration 14, loss = 0.00826396\n",
      "Iteration 15, loss = 0.00710570\n",
      "Iteration 16, loss = 0.00698685\n",
      "Iteration 17, loss = 0.00619541\n",
      "Iteration 18, loss = 0.00590374\n",
      "Iteration 19, loss = 0.00550175\n",
      "Iteration 20, loss = 0.00482865\n",
      "Iteration 21, loss = 0.00476402\n",
      "Iteration 22, loss = 0.00447284\n",
      "Iteration 23, loss = 0.00432566\n",
      "Iteration 24, loss = 0.00395242\n",
      "Iteration 25, loss = 0.00367340\n",
      "Iteration 26, loss = 0.00399336\n",
      "Iteration 27, loss = 0.00388948\n",
      "Iteration 28, loss = 0.00354819\n",
      "Iteration 29, loss = 0.00342317\n",
      "Iteration 30, loss = 0.00315880\n",
      "Iteration 31, loss = 0.00270436\n",
      "Iteration 32, loss = 0.00271700\n",
      "Iteration 33, loss = 0.00241022\n",
      "Iteration 34, loss = 0.00255842\n",
      "Iteration 35, loss = 0.00231685\n",
      "Iteration 36, loss = 0.00223086\n",
      "Iteration 37, loss = 0.00205547\n",
      "Iteration 38, loss = 0.00197200\n",
      "Iteration 39, loss = 0.00214098\n",
      "Iteration 40, loss = 0.00177421\n",
      "Iteration 41, loss = 0.00327840\n",
      "Iteration 42, loss = 0.00203173\n",
      "Iteration 43, loss = 0.00172287\n",
      "Iteration 44, loss = 0.00175120\n",
      "Iteration 45, loss = 0.00156376\n",
      "Iteration 46, loss = 0.00148551\n",
      "Iteration 47, loss = 0.00160228\n",
      "Iteration 48, loss = 0.00179767\n",
      "Iteration 49, loss = 0.00139269\n",
      "Iteration 50, loss = 0.00201290\n",
      "Iteration 51, loss = 0.00245485\n",
      "Iteration 52, loss = 0.00342123\n",
      "Iteration 53, loss = 0.00296950\n",
      "Iteration 54, loss = 0.00314604\n",
      "Iteration 55, loss = 0.00245881\n",
      "Iteration 56, loss = 0.00121656\n",
      "Iteration 57, loss = 0.00136918\n",
      "Iteration 58, loss = 0.00137104\n",
      "Iteration 59, loss = 0.00133301\n",
      "Iteration 60, loss = 0.00103302\n",
      "Iteration 61, loss = 0.00110680\n",
      "Iteration 62, loss = 0.00091890\n",
      "Iteration 63, loss = 0.00095545\n",
      "Iteration 64, loss = 0.00091168\n",
      "Iteration 65, loss = 0.00088712\n",
      "Iteration 66, loss = 0.00085967\n",
      "Iteration 67, loss = 0.00084945\n",
      "Iteration 68, loss = 0.00079127\n",
      "Iteration 69, loss = 0.00081017\n",
      "Iteration 70, loss = 0.00074857\n",
      "Iteration 71, loss = 0.00075090\n",
      "Iteration 72, loss = 0.00070805\n",
      "Iteration 73, loss = 0.00072461\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31797792\n",
      "Iteration 2, loss = 0.08714779\n",
      "Iteration 3, loss = 0.06455932\n",
      "Iteration 4, loss = 0.05354799\n",
      "Iteration 5, loss = 0.04343649\n",
      "Iteration 6, loss = 0.03551782\n",
      "Iteration 7, loss = 0.03082093\n",
      "Iteration 8, loss = 0.02837858\n",
      "Iteration 9, loss = 0.02468394\n",
      "Iteration 10, loss = 0.02211203\n",
      "Iteration 11, loss = 0.01970202\n",
      "Iteration 12, loss = 0.01800469\n",
      "Iteration 13, loss = 0.01644876\n",
      "Iteration 14, loss = 0.01509569\n",
      "Iteration 15, loss = 0.01356429\n",
      "Iteration 16, loss = 0.01280751\n",
      "Iteration 17, loss = 0.01162788\n",
      "Iteration 18, loss = 0.01063567\n",
      "Iteration 19, loss = 0.01011393\n",
      "Iteration 20, loss = 0.00936781\n",
      "Iteration 21, loss = 0.00895954\n",
      "Iteration 22, loss = 0.00791266\n",
      "Iteration 23, loss = 0.00756263\n",
      "Iteration 24, loss = 0.00712343\n",
      "Iteration 25, loss = 0.00661140\n",
      "Iteration 26, loss = 0.00613179\n",
      "Iteration 27, loss = 0.00581380\n",
      "Iteration 28, loss = 0.00547137\n",
      "Iteration 29, loss = 0.00527764\n",
      "Iteration 30, loss = 0.00497396\n",
      "Iteration 31, loss = 0.00462457\n",
      "Iteration 32, loss = 0.00443483\n",
      "Iteration 33, loss = 0.00431875\n",
      "Iteration 34, loss = 0.00402284\n",
      "Iteration 35, loss = 0.00379965\n",
      "Iteration 36, loss = 0.00379257\n",
      "Iteration 37, loss = 0.00350254\n",
      "Iteration 38, loss = 0.00350619\n",
      "Iteration 39, loss = 0.00325173\n",
      "Iteration 40, loss = 0.00310024\n",
      "Iteration 41, loss = 0.00304808\n",
      "Iteration 42, loss = 0.00291511\n",
      "Iteration 43, loss = 0.00288691\n",
      "Iteration 44, loss = 0.00285061\n",
      "Iteration 45, loss = 0.00296678\n",
      "Iteration 46, loss = 0.00276221\n",
      "Iteration 47, loss = 0.00260503\n",
      "Iteration 48, loss = 0.00244927\n",
      "Iteration 49, loss = 0.00248123\n",
      "Iteration 50, loss = 0.00226143\n",
      "Iteration 51, loss = 0.00224282\n",
      "Iteration 52, loss = 0.00205139\n",
      "Iteration 53, loss = 0.00221199\n",
      "Iteration 54, loss = 0.00237876\n",
      "Iteration 55, loss = 0.00189132\n",
      "Iteration 56, loss = 0.00246898\n",
      "Iteration 57, loss = 0.00170960\n",
      "Iteration 58, loss = 0.00363395\n",
      "Iteration 59, loss = 0.00518019\n",
      "Iteration 60, loss = 0.00218707\n",
      "Iteration 61, loss = 0.00243850\n",
      "Iteration 62, loss = 0.00203273\n",
      "Iteration 63, loss = 0.00210192\n",
      "Iteration 64, loss = 0.00195632\n",
      "Iteration 65, loss = 0.00171557\n",
      "Iteration 66, loss = 0.00181574\n",
      "Iteration 67, loss = 0.00152320\n",
      "Iteration 68, loss = 0.00151294\n",
      "Iteration 69, loss = 0.00140483\n",
      "Iteration 70, loss = 0.00147887\n",
      "Iteration 71, loss = 0.00135298\n",
      "Iteration 72, loss = 0.00130552\n",
      "Iteration 73, loss = 0.00136144\n",
      "Iteration 74, loss = 0.00124846\n",
      "Iteration 75, loss = 0.00124214\n",
      "Iteration 76, loss = 0.00118045\n",
      "Iteration 77, loss = 0.00119893\n",
      "Iteration 78, loss = 0.00119231\n",
      "Iteration 79, loss = 0.00117453\n",
      "Iteration 80, loss = 0.00130421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38946840\n",
      "Iteration 2, loss = 0.10152677\n",
      "Iteration 3, loss = 0.06628138\n",
      "Iteration 4, loss = 0.05179371\n",
      "Iteration 5, loss = 0.04167283\n",
      "Iteration 6, loss = 0.03446043\n",
      "Iteration 7, loss = 0.03009108\n",
      "Iteration 8, loss = 0.02669857\n",
      "Iteration 9, loss = 0.02342534\n",
      "Iteration 10, loss = 0.02110314\n",
      "Iteration 11, loss = 0.01912775\n",
      "Iteration 12, loss = 0.01745944\n",
      "Iteration 13, loss = 0.01591681\n",
      "Iteration 14, loss = 0.01477892\n",
      "Iteration 15, loss = 0.01319263\n",
      "Iteration 16, loss = 0.01214917\n",
      "Iteration 17, loss = 0.01129908\n",
      "Iteration 18, loss = 0.01028399\n",
      "Iteration 19, loss = 0.00974687\n",
      "Iteration 20, loss = 0.00874561\n",
      "Iteration 21, loss = 0.00820457\n",
      "Iteration 22, loss = 0.00769882\n",
      "Iteration 23, loss = 0.00703490\n",
      "Iteration 24, loss = 0.00667576\n",
      "Iteration 25, loss = 0.00623440\n",
      "Iteration 26, loss = 0.00603423\n",
      "Iteration 27, loss = 0.00569674\n",
      "Iteration 28, loss = 0.00533541\n",
      "Iteration 29, loss = 0.00506804\n",
      "Iteration 30, loss = 0.00488510\n",
      "Iteration 31, loss = 0.00456624\n",
      "Iteration 32, loss = 0.00426707\n",
      "Iteration 33, loss = 0.00405241\n",
      "Iteration 34, loss = 0.00391212\n",
      "Iteration 35, loss = 0.00377373\n",
      "Iteration 36, loss = 0.00368080\n",
      "Iteration 37, loss = 0.00344467\n",
      "Iteration 38, loss = 0.00337002\n",
      "Iteration 39, loss = 0.00362151\n",
      "Iteration 40, loss = 0.00287824\n",
      "Iteration 41, loss = 0.00327245\n",
      "Iteration 42, loss = 0.00290144\n",
      "Iteration 43, loss = 0.00279543\n",
      "Iteration 44, loss = 0.00267657\n",
      "Iteration 45, loss = 0.00287697\n",
      "Iteration 46, loss = 0.00242177\n",
      "Iteration 47, loss = 0.00250221\n",
      "Iteration 48, loss = 0.00231911\n",
      "Iteration 49, loss = 0.00223468\n",
      "Iteration 50, loss = 0.00225168\n",
      "Iteration 51, loss = 0.00224935\n",
      "Iteration 52, loss = 0.00214745\n",
      "Iteration 53, loss = 0.00206871\n",
      "Iteration 54, loss = 0.00196371\n",
      "Iteration 55, loss = 0.00180767\n",
      "Iteration 56, loss = 0.00211936\n",
      "Iteration 57, loss = 0.00176834\n",
      "Iteration 58, loss = 0.00183167\n",
      "Iteration 59, loss = 0.00172848\n",
      "Iteration 60, loss = 0.00162401\n",
      "Iteration 61, loss = 0.00169436\n",
      "Iteration 62, loss = 0.00156548\n",
      "Iteration 63, loss = 0.00148446\n",
      "Iteration 64, loss = 0.00150733\n",
      "Iteration 65, loss = 0.00151632\n",
      "Iteration 66, loss = 0.00165071\n",
      "Iteration 67, loss = 0.00146946\n",
      "Iteration 68, loss = 0.00156333\n",
      "Iteration 69, loss = 0.00133965\n",
      "Iteration 70, loss = 0.00130575\n",
      "Iteration 71, loss = 0.00137480\n",
      "Iteration 72, loss = 0.00150364\n",
      "Iteration 73, loss = 0.00141716\n",
      "Iteration 74, loss = 0.00132535\n",
      "Iteration 75, loss = 0.00114373\n",
      "Iteration 76, loss = 0.00120801\n",
      "Iteration 77, loss = 0.00109365\n",
      "Iteration 78, loss = 0.00117858\n",
      "Iteration 79, loss = 0.00118284\n",
      "Iteration 80, loss = 0.00106935\n",
      "Iteration 81, loss = 0.00099041\n",
      "Iteration 82, loss = 0.00142203\n",
      "Iteration 83, loss = 0.00090822\n",
      "Iteration 84, loss = 0.00104053\n",
      "Iteration 85, loss = 0.00100435\n",
      "Iteration 86, loss = 0.00092618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43922891\n",
      "Iteration 2, loss = 0.09989111\n",
      "Iteration 3, loss = 0.06704556\n",
      "Iteration 4, loss = 0.05328108\n",
      "Iteration 5, loss = 0.04336755\n",
      "Iteration 6, loss = 0.03661670\n",
      "Iteration 7, loss = 0.03138996\n",
      "Iteration 8, loss = 0.02715762\n",
      "Iteration 9, loss = 0.02453207\n",
      "Iteration 10, loss = 0.02185930\n",
      "Iteration 11, loss = 0.01997291\n",
      "Iteration 12, loss = 0.01814884\n",
      "Iteration 13, loss = 0.01669203\n",
      "Iteration 14, loss = 0.01518209\n",
      "Iteration 15, loss = 0.01436868\n",
      "Iteration 16, loss = 0.01310259\n",
      "Iteration 17, loss = 0.01240495\n",
      "Iteration 18, loss = 0.01134507\n",
      "Iteration 19, loss = 0.01041137\n",
      "Iteration 20, loss = 0.00990364\n",
      "Iteration 21, loss = 0.00927080\n",
      "Iteration 22, loss = 0.00843379\n",
      "Iteration 23, loss = 0.00789020\n",
      "Iteration 24, loss = 0.00751403\n",
      "Iteration 25, loss = 0.00690865\n",
      "Iteration 26, loss = 0.00655649\n",
      "Iteration 27, loss = 0.00608032\n",
      "Iteration 28, loss = 0.00584804\n",
      "Iteration 29, loss = 0.00550244\n",
      "Iteration 30, loss = 0.00524571\n",
      "Iteration 31, loss = 0.00499974\n",
      "Iteration 32, loss = 0.00469377\n",
      "Iteration 33, loss = 0.00455304\n",
      "Iteration 34, loss = 0.00447801\n",
      "Iteration 35, loss = 0.00421009\n",
      "Iteration 36, loss = 0.00395030\n",
      "Iteration 37, loss = 0.00378010\n",
      "Iteration 38, loss = 0.00371779\n",
      "Iteration 39, loss = 0.00338747\n",
      "Iteration 40, loss = 0.00350801\n",
      "Iteration 41, loss = 0.00362430\n",
      "Iteration 42, loss = 0.00310197\n",
      "Iteration 43, loss = 0.00303185\n",
      "Iteration 44, loss = 0.00303539\n",
      "Iteration 45, loss = 0.00287489\n",
      "Iteration 46, loss = 0.00282044\n",
      "Iteration 47, loss = 0.00260860\n",
      "Iteration 48, loss = 0.00261484\n",
      "Iteration 49, loss = 0.00244795\n",
      "Iteration 50, loss = 0.00240028\n",
      "Iteration 51, loss = 0.00235711\n",
      "Iteration 52, loss = 0.00224348\n",
      "Iteration 53, loss = 0.00226076\n",
      "Iteration 54, loss = 0.00212448\n",
      "Iteration 55, loss = 0.00226777\n",
      "Iteration 56, loss = 0.00205271\n",
      "Iteration 57, loss = 0.00198619\n",
      "Iteration 58, loss = 0.00190784\n",
      "Iteration 59, loss = 0.00211126\n",
      "Iteration 60, loss = 0.00176223\n",
      "Iteration 61, loss = 0.00194983\n",
      "Iteration 62, loss = 0.00204766\n",
      "Iteration 63, loss = 0.00165608\n",
      "Iteration 64, loss = 0.00162062\n",
      "Iteration 65, loss = 0.00172851\n",
      "Iteration 66, loss = 0.00166189\n",
      "Iteration 67, loss = 0.00158576\n",
      "Iteration 68, loss = 0.00153346\n",
      "Iteration 69, loss = 0.00149798\n",
      "Iteration 70, loss = 0.00146873\n",
      "Iteration 71, loss = 0.00135540\n",
      "Iteration 72, loss = 0.00140737\n",
      "Iteration 73, loss = 0.00133622\n",
      "Iteration 74, loss = 0.00137622\n",
      "Iteration 75, loss = 0.00128630\n",
      "Iteration 76, loss = 0.00160446\n",
      "Iteration 77, loss = 0.00119334\n",
      "Iteration 78, loss = 0.00128240\n",
      "Iteration 79, loss = 0.00118652\n",
      "Iteration 80, loss = 0.00117979\n",
      "Iteration 81, loss = 0.00113918\n",
      "Iteration 82, loss = 0.00112646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20619223\n",
      "Iteration 2, loss = 0.07504941\n",
      "Iteration 3, loss = 0.05505925\n",
      "Iteration 4, loss = 0.04289291\n",
      "Iteration 5, loss = 0.03543918\n",
      "Iteration 6, loss = 0.02979527\n",
      "Iteration 7, loss = 0.02704550\n",
      "Iteration 8, loss = 0.02275370\n",
      "Iteration 9, loss = 0.01970961\n",
      "Iteration 10, loss = 0.01821162\n",
      "Iteration 11, loss = 0.01560744\n",
      "Iteration 12, loss = 0.01411517\n",
      "Iteration 13, loss = 0.01313756\n",
      "Iteration 14, loss = 0.01189572\n",
      "Iteration 15, loss = 0.01038104\n",
      "Iteration 16, loss = 0.00944958\n",
      "Iteration 17, loss = 0.00853266\n",
      "Iteration 18, loss = 0.00780720\n",
      "Iteration 19, loss = 0.00718261\n",
      "Iteration 20, loss = 0.00663348\n",
      "Iteration 21, loss = 0.00621223\n",
      "Iteration 22, loss = 0.00571432\n",
      "Iteration 23, loss = 0.00542311\n",
      "Iteration 24, loss = 0.00509230\n",
      "Iteration 25, loss = 0.00467689\n",
      "Iteration 26, loss = 0.00444016\n",
      "Iteration 27, loss = 0.00414765\n",
      "Iteration 28, loss = 0.00390053\n",
      "Iteration 29, loss = 0.00366059\n",
      "Iteration 30, loss = 0.00347246\n",
      "Iteration 31, loss = 0.00327878\n",
      "Iteration 32, loss = 0.00317002\n",
      "Iteration 33, loss = 0.00296929\n",
      "Iteration 34, loss = 0.00285738\n",
      "Iteration 35, loss = 0.00281505\n",
      "Iteration 36, loss = 0.00266948\n",
      "Iteration 37, loss = 0.00250484\n",
      "Iteration 38, loss = 0.00259440\n",
      "Iteration 39, loss = 0.00257499\n",
      "Iteration 40, loss = 0.00301449\n",
      "Iteration 41, loss = 0.00272537\n",
      "Iteration 42, loss = 0.00246845\n",
      "Iteration 43, loss = 0.00202694\n",
      "Iteration 44, loss = 0.00240595\n",
      "Iteration 45, loss = 0.00208483\n",
      "Iteration 46, loss = 0.00187447\n",
      "Iteration 47, loss = 0.00181684\n",
      "Iteration 48, loss = 0.00174045\n",
      "Iteration 49, loss = 0.00165327\n",
      "Iteration 50, loss = 0.00167837\n",
      "Iteration 51, loss = 0.00156530\n",
      "Iteration 52, loss = 0.00151234\n",
      "Iteration 53, loss = 0.00148111\n",
      "Iteration 54, loss = 0.00142844\n",
      "Iteration 55, loss = 0.00145161\n",
      "Iteration 56, loss = 0.00135603\n",
      "Iteration 57, loss = 0.00141201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22021099\n",
      "Iteration 2, loss = 0.07558898\n",
      "Iteration 3, loss = 0.05419929\n",
      "Iteration 4, loss = 0.04323026\n",
      "Iteration 5, loss = 0.03452065\n",
      "Iteration 6, loss = 0.02974093\n",
      "Iteration 7, loss = 0.02649318\n",
      "Iteration 8, loss = 0.02335194\n",
      "Iteration 9, loss = 0.02036602\n",
      "Iteration 10, loss = 0.01794102\n",
      "Iteration 11, loss = 0.01559221\n",
      "Iteration 12, loss = 0.01452646\n",
      "Iteration 13, loss = 0.01299275\n",
      "Iteration 14, loss = 0.01173380\n",
      "Iteration 15, loss = 0.01034209\n",
      "Iteration 16, loss = 0.00987366\n",
      "Iteration 17, loss = 0.00936955\n",
      "Iteration 18, loss = 0.00778034\n",
      "Iteration 19, loss = 0.00711450\n",
      "Iteration 20, loss = 0.00649094\n",
      "Iteration 21, loss = 0.00652115\n",
      "Iteration 22, loss = 0.00607168\n",
      "Iteration 23, loss = 0.00511286\n",
      "Iteration 24, loss = 0.00471604\n",
      "Iteration 25, loss = 0.00430820\n",
      "Iteration 26, loss = 0.00396651\n",
      "Iteration 27, loss = 0.00369128\n",
      "Iteration 28, loss = 0.00349680\n",
      "Iteration 29, loss = 0.00333067\n",
      "Iteration 30, loss = 0.00304225\n",
      "Iteration 31, loss = 0.00349483\n",
      "Iteration 32, loss = 0.00388475\n",
      "Iteration 33, loss = 0.00292893\n",
      "Iteration 34, loss = 0.00272563\n",
      "Iteration 35, loss = 0.00242697\n",
      "Iteration 36, loss = 0.00218500\n",
      "Iteration 37, loss = 0.00205471\n",
      "Iteration 38, loss = 0.00195409\n",
      "Iteration 39, loss = 0.00180293\n",
      "Iteration 40, loss = 0.00171410\n",
      "Iteration 41, loss = 0.00173883\n",
      "Iteration 42, loss = 0.00166727\n",
      "Iteration 43, loss = 0.00149498\n",
      "Iteration 44, loss = 0.00141770\n",
      "Iteration 45, loss = 0.00136137\n",
      "Iteration 46, loss = 0.00129053\n",
      "Iteration 47, loss = 0.00124221\n",
      "Iteration 48, loss = 0.00120627\n",
      "Iteration 49, loss = 0.00115860\n",
      "Iteration 50, loss = 0.00111373\n",
      "Iteration 51, loss = 0.00108600\n",
      "Iteration 52, loss = 0.00104564\n",
      "Iteration 53, loss = 0.00101725\n",
      "Iteration 54, loss = 0.00099994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48178724\n",
      "Iteration 2, loss = 0.11052810\n",
      "Iteration 3, loss = 0.07108988\n",
      "Iteration 4, loss = 0.05382695\n",
      "Iteration 5, loss = 0.04513897\n",
      "Iteration 6, loss = 0.04020810\n",
      "Iteration 7, loss = 0.03572459\n",
      "Iteration 8, loss = 0.03277038\n",
      "Iteration 9, loss = 0.02832831\n",
      "Iteration 10, loss = 0.02621820\n",
      "Iteration 11, loss = 0.02409591\n",
      "Iteration 12, loss = 0.02215660\n",
      "Iteration 13, loss = 0.02057962\n",
      "Iteration 14, loss = 0.01955454\n",
      "Iteration 15, loss = 0.01757894\n",
      "Iteration 16, loss = 0.01633089\n",
      "Iteration 17, loss = 0.01633148\n",
      "Iteration 18, loss = 0.01531113\n",
      "Iteration 19, loss = 0.01400352\n",
      "Iteration 20, loss = 0.01305481\n",
      "Iteration 21, loss = 0.01217665\n",
      "Iteration 22, loss = 0.01138771\n",
      "Iteration 23, loss = 0.01076082\n",
      "Iteration 24, loss = 0.01016953\n",
      "Iteration 25, loss = 0.00976988\n",
      "Iteration 26, loss = 0.00903212\n",
      "Iteration 27, loss = 0.00858361\n",
      "Iteration 28, loss = 0.00815491\n",
      "Iteration 29, loss = 0.00780117\n",
      "Iteration 30, loss = 0.00732009\n",
      "Iteration 31, loss = 0.00683912\n",
      "Iteration 32, loss = 0.00642234\n",
      "Iteration 33, loss = 0.00613338\n",
      "Iteration 34, loss = 0.00596021\n",
      "Iteration 35, loss = 0.00565494\n",
      "Iteration 36, loss = 0.00522638\n",
      "Iteration 37, loss = 0.00497050\n",
      "Iteration 38, loss = 0.00478993\n",
      "Iteration 39, loss = 0.00452552\n",
      "Iteration 40, loss = 0.00426352\n",
      "Iteration 41, loss = 0.00447739\n",
      "Iteration 42, loss = 0.00428971\n",
      "Iteration 43, loss = 0.00384667\n",
      "Iteration 44, loss = 0.00378149\n",
      "Iteration 45, loss = 0.00373687\n",
      "Iteration 46, loss = 0.00351898\n",
      "Iteration 47, loss = 0.00366651\n",
      "Iteration 48, loss = 0.00341652\n",
      "Iteration 49, loss = 0.00576711\n",
      "Iteration 50, loss = 0.00594678\n",
      "Iteration 51, loss = 0.00370052\n",
      "Iteration 52, loss = 0.00301629\n",
      "Iteration 53, loss = 0.00311369\n",
      "Iteration 54, loss = 0.00319263\n",
      "Iteration 55, loss = 0.00253793\n",
      "Iteration 56, loss = 0.00227985\n",
      "Iteration 57, loss = 0.00211245\n",
      "Iteration 58, loss = 0.00196425\n",
      "Iteration 59, loss = 0.00188896\n",
      "Iteration 60, loss = 0.00251739\n",
      "Iteration 61, loss = 0.00346042\n",
      "Iteration 62, loss = 0.00272830\n",
      "Iteration 63, loss = 0.00187335\n",
      "Iteration 64, loss = 0.00174014\n",
      "Iteration 65, loss = 0.00162311\n",
      "Iteration 66, loss = 0.00209215\n",
      "Iteration 67, loss = 0.00191040\n",
      "Iteration 68, loss = 0.00152166\n",
      "Iteration 69, loss = 0.00141739\n",
      "Iteration 70, loss = 0.00132384\n",
      "Iteration 71, loss = 0.00128857\n",
      "Iteration 72, loss = 0.00125035\n",
      "Iteration 73, loss = 0.00121626\n",
      "Iteration 74, loss = 0.00117975\n",
      "Iteration 75, loss = 0.00115972\n",
      "Iteration 76, loss = 0.00112071\n",
      "Iteration 77, loss = 0.00110057\n",
      "Iteration 78, loss = 0.00107166\n",
      "Iteration 79, loss = 0.00104466\n",
      "Iteration 80, loss = 0.00102286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31154218\n",
      "Iteration 2, loss = 0.09261454\n",
      "Iteration 3, loss = 0.06410551\n",
      "Iteration 4, loss = 0.04891770\n",
      "Iteration 5, loss = 0.03989712\n",
      "Iteration 6, loss = 0.03392720\n",
      "Iteration 7, loss = 0.02946285\n",
      "Iteration 8, loss = 0.02578149\n",
      "Iteration 9, loss = 0.02309109\n",
      "Iteration 10, loss = 0.02095445\n",
      "Iteration 11, loss = 0.01918059\n",
      "Iteration 12, loss = 0.01742134\n",
      "Iteration 13, loss = 0.01634071\n",
      "Iteration 14, loss = 0.01485408\n",
      "Iteration 15, loss = 0.01369672\n",
      "Iteration 16, loss = 0.01245511\n",
      "Iteration 17, loss = 0.01138748\n",
      "Iteration 18, loss = 0.01107917\n",
      "Iteration 19, loss = 0.01016289\n",
      "Iteration 20, loss = 0.00939898\n",
      "Iteration 21, loss = 0.00850351\n",
      "Iteration 22, loss = 0.00791696\n",
      "Iteration 23, loss = 0.00742202\n",
      "Iteration 24, loss = 0.00697265\n",
      "Iteration 25, loss = 0.00646502\n",
      "Iteration 26, loss = 0.00606773\n",
      "Iteration 27, loss = 0.00580020\n",
      "Iteration 28, loss = 0.00533426\n",
      "Iteration 29, loss = 0.00515340\n",
      "Iteration 30, loss = 0.00509481\n",
      "Iteration 31, loss = 0.00507251\n",
      "Iteration 32, loss = 0.00478219\n",
      "Iteration 33, loss = 0.00420865\n",
      "Iteration 34, loss = 0.00395530\n",
      "Iteration 35, loss = 0.00388147\n",
      "Iteration 36, loss = 0.00367255\n",
      "Iteration 37, loss = 0.00381011\n",
      "Iteration 38, loss = 0.00368126\n",
      "Iteration 39, loss = 0.00351653\n",
      "Iteration 40, loss = 0.00295158\n",
      "Iteration 41, loss = 0.00321539\n",
      "Iteration 42, loss = 0.00270091\n",
      "Iteration 43, loss = 0.00266828\n",
      "Iteration 44, loss = 0.00249112\n",
      "Iteration 45, loss = 0.00243443\n",
      "Iteration 46, loss = 0.00232488\n",
      "Iteration 47, loss = 0.00251949\n",
      "Iteration 48, loss = 0.00227082\n",
      "Iteration 49, loss = 0.00222821\n",
      "Iteration 50, loss = 0.00199673\n",
      "Iteration 51, loss = 0.00202264\n",
      "Iteration 52, loss = 0.00198522\n",
      "Iteration 53, loss = 0.00186562\n",
      "Iteration 54, loss = 0.00177058\n",
      "Iteration 55, loss = 0.00172473\n",
      "Iteration 56, loss = 0.00182975\n",
      "Iteration 57, loss = 0.00184133\n",
      "Iteration 58, loss = 0.00167892\n",
      "Iteration 59, loss = 0.00156564\n",
      "Iteration 60, loss = 0.00158868\n",
      "Iteration 61, loss = 0.00146083\n",
      "Iteration 62, loss = 0.00135696\n",
      "Iteration 63, loss = 0.00152001\n",
      "Iteration 64, loss = 0.00134309\n",
      "Iteration 65, loss = 0.00138767\n",
      "Iteration 66, loss = 0.00174687\n",
      "Iteration 67, loss = 0.00242154\n",
      "Iteration 68, loss = 0.00187930\n",
      "Iteration 69, loss = 0.00159537\n",
      "Iteration 70, loss = 0.00160917\n",
      "Iteration 71, loss = 0.00105540\n",
      "Iteration 72, loss = 0.00119820\n",
      "Iteration 73, loss = 0.00110224\n",
      "Iteration 74, loss = 0.00109293\n",
      "Iteration 75, loss = 0.00108799\n",
      "Iteration 76, loss = 0.00102522\n",
      "Iteration 77, loss = 0.00095039\n",
      "Iteration 78, loss = 0.00094386\n",
      "Iteration 79, loss = 0.00091807\n",
      "Iteration 80, loss = 0.00092514\n",
      "Iteration 81, loss = 0.00088626\n",
      "Iteration 82, loss = 0.00087265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35135725\n",
      "Iteration 2, loss = 0.10010452\n",
      "Iteration 3, loss = 0.07018731\n",
      "Iteration 4, loss = 0.05340612\n",
      "Iteration 5, loss = 0.04469526\n",
      "Iteration 6, loss = 0.03725162\n",
      "Iteration 7, loss = 0.03077751\n",
      "Iteration 8, loss = 0.02882202\n",
      "Iteration 9, loss = 0.02505058\n",
      "Iteration 10, loss = 0.02196559\n",
      "Iteration 11, loss = 0.02016720\n",
      "Iteration 12, loss = 0.01800753\n",
      "Iteration 13, loss = 0.01655589\n",
      "Iteration 14, loss = 0.01516591\n",
      "Iteration 15, loss = 0.01388195\n",
      "Iteration 16, loss = 0.01256730\n",
      "Iteration 17, loss = 0.01184626\n",
      "Iteration 18, loss = 0.01088493\n",
      "Iteration 19, loss = 0.01042885\n",
      "Iteration 20, loss = 0.00980350\n",
      "Iteration 21, loss = 0.00894315\n",
      "Iteration 22, loss = 0.00816221\n",
      "Iteration 23, loss = 0.00753092\n",
      "Iteration 24, loss = 0.00709942\n",
      "Iteration 25, loss = 0.00660440\n",
      "Iteration 26, loss = 0.00622668\n",
      "Iteration 27, loss = 0.00596252\n",
      "Iteration 28, loss = 0.00564880\n",
      "Iteration 29, loss = 0.00526205\n",
      "Iteration 30, loss = 0.00530931\n",
      "Iteration 31, loss = 0.00483293\n",
      "Iteration 32, loss = 0.00485750\n",
      "Iteration 33, loss = 0.00431415\n",
      "Iteration 34, loss = 0.00437988\n",
      "Iteration 35, loss = 0.00395392\n",
      "Iteration 36, loss = 0.00382557\n",
      "Iteration 37, loss = 0.00388488\n",
      "Iteration 38, loss = 0.00344152\n",
      "Iteration 39, loss = 0.00350799\n",
      "Iteration 40, loss = 0.00317931\n",
      "Iteration 41, loss = 0.00317328\n",
      "Iteration 42, loss = 0.00292120\n",
      "Iteration 43, loss = 0.00295187\n",
      "Iteration 44, loss = 0.00280344\n",
      "Iteration 45, loss = 0.00264790\n",
      "Iteration 46, loss = 0.00260272\n",
      "Iteration 47, loss = 0.00258362\n",
      "Iteration 48, loss = 0.00250691\n",
      "Iteration 49, loss = 0.00242400\n",
      "Iteration 50, loss = 0.00229766\n",
      "Iteration 51, loss = 0.00218838\n",
      "Iteration 52, loss = 0.00216561\n",
      "Iteration 53, loss = 0.00221490\n",
      "Iteration 54, loss = 0.00198914\n",
      "Iteration 55, loss = 0.00209688\n",
      "Iteration 56, loss = 0.00195260\n",
      "Iteration 57, loss = 0.00192179\n",
      "Iteration 58, loss = 0.00180075\n",
      "Iteration 59, loss = 0.00184116\n",
      "Iteration 60, loss = 0.00174305\n",
      "Iteration 61, loss = 0.00168633\n",
      "Iteration 62, loss = 0.00164827\n",
      "Iteration 63, loss = 0.00157846\n",
      "Iteration 64, loss = 0.00157590\n",
      "Iteration 65, loss = 0.00151861\n",
      "Iteration 66, loss = 0.00150370\n",
      "Iteration 67, loss = 0.00144656\n",
      "Iteration 68, loss = 0.00141025\n",
      "Iteration 69, loss = 0.00138145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65724621\n",
      "Iteration 2, loss = 0.15575559\n",
      "Iteration 3, loss = 0.09259720\n",
      "Iteration 4, loss = 0.06740731\n",
      "Iteration 5, loss = 0.05602829\n",
      "Iteration 6, loss = 0.04554500\n",
      "Iteration 7, loss = 0.03958497\n",
      "Iteration 8, loss = 0.03526033\n",
      "Iteration 9, loss = 0.03191449\n",
      "Iteration 10, loss = 0.02906176\n",
      "Iteration 11, loss = 0.02654725\n",
      "Iteration 12, loss = 0.02494324\n",
      "Iteration 13, loss = 0.02277990\n",
      "Iteration 14, loss = 0.02144807\n",
      "Iteration 15, loss = 0.01968156\n",
      "Iteration 16, loss = 0.01854272\n",
      "Iteration 17, loss = 0.01734459\n",
      "Iteration 18, loss = 0.01613235\n",
      "Iteration 19, loss = 0.01515610\n",
      "Iteration 20, loss = 0.01448736\n",
      "Iteration 21, loss = 0.01343531\n",
      "Iteration 22, loss = 0.01263300\n",
      "Iteration 23, loss = 0.01198105\n",
      "Iteration 24, loss = 0.01128117\n",
      "Iteration 25, loss = 0.01060369\n",
      "Iteration 26, loss = 0.00998370\n",
      "Iteration 27, loss = 0.00940913\n",
      "Iteration 28, loss = 0.00891278\n",
      "Iteration 29, loss = 0.00847388\n",
      "Iteration 30, loss = 0.00798537\n",
      "Iteration 31, loss = 0.00757477\n",
      "Iteration 32, loss = 0.00719923\n",
      "Iteration 33, loss = 0.00680855\n",
      "Iteration 34, loss = 0.00646472\n",
      "Iteration 35, loss = 0.00622589\n",
      "Iteration 36, loss = 0.00596216\n",
      "Iteration 37, loss = 0.00584598\n",
      "Iteration 38, loss = 0.00549945\n",
      "Iteration 39, loss = 0.00516653\n",
      "Iteration 40, loss = 0.00506669\n",
      "Iteration 41, loss = 0.00477591\n",
      "Iteration 42, loss = 0.00462863\n",
      "Iteration 43, loss = 0.00443789\n",
      "Iteration 44, loss = 0.00430170\n",
      "Iteration 45, loss = 0.00415261\n",
      "Iteration 46, loss = 0.00396698\n",
      "Iteration 47, loss = 0.00398090\n",
      "Iteration 48, loss = 0.00357273\n",
      "Iteration 49, loss = 0.00382507\n",
      "Iteration 50, loss = 0.00360285\n",
      "Iteration 51, loss = 0.00345774\n",
      "Iteration 52, loss = 0.00319668\n",
      "Iteration 53, loss = 0.00313334\n",
      "Iteration 54, loss = 0.00307376\n",
      "Iteration 55, loss = 0.00291197\n",
      "Iteration 56, loss = 0.00291204\n",
      "Iteration 57, loss = 0.00279898\n",
      "Iteration 58, loss = 0.00267186\n",
      "Iteration 59, loss = 0.00260657\n",
      "Iteration 60, loss = 0.00256332\n",
      "Iteration 61, loss = 0.00240754\n",
      "Iteration 62, loss = 0.00241177\n",
      "Iteration 63, loss = 0.00236491\n",
      "Iteration 64, loss = 0.00213205\n",
      "Iteration 65, loss = 0.00245999\n",
      "Iteration 66, loss = 0.00216093\n",
      "Iteration 67, loss = 0.00208164\n",
      "Iteration 68, loss = 0.00215853\n",
      "Iteration 69, loss = 0.00201862\n",
      "Iteration 70, loss = 0.00202284\n",
      "Iteration 71, loss = 0.00191767\n",
      "Iteration 72, loss = 0.00223117\n",
      "Iteration 73, loss = 0.00202180\n",
      "Iteration 74, loss = 0.00252296\n",
      "Iteration 75, loss = 0.00176497\n",
      "Iteration 76, loss = 0.00206276\n",
      "Iteration 77, loss = 0.00178465\n",
      "Iteration 78, loss = 0.00178521\n",
      "Iteration 79, loss = 0.00165750\n",
      "Iteration 80, loss = 0.00156269\n",
      "Iteration 81, loss = 0.00160558\n",
      "Iteration 82, loss = 0.00161952\n",
      "Iteration 83, loss = 0.00148685\n",
      "Iteration 84, loss = 0.00146550\n",
      "Iteration 85, loss = 0.00140932\n",
      "Iteration 86, loss = 0.00143781\n",
      "Iteration 87, loss = 0.00152936\n",
      "Iteration 88, loss = 0.00143325\n",
      "Iteration 89, loss = 0.00141991\n",
      "Iteration 90, loss = 0.00130458\n",
      "Iteration 91, loss = 0.00135263\n",
      "Iteration 92, loss = 0.00130595\n",
      "Iteration 93, loss = 0.00155447\n",
      "Iteration 94, loss = 0.00126090\n",
      "Iteration 95, loss = 0.00124817\n",
      "Iteration 96, loss = 0.00118391\n",
      "Iteration 97, loss = 0.00113730\n",
      "Iteration 98, loss = 0.00112721\n",
      "Iteration 99, loss = 0.00108479\n",
      "Iteration 100, loss = 0.00108258\n",
      "Iteration 101, loss = 0.00105688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "test_roc2=[]\n",
    "test_prc2=[]\n",
    "test_roc22=[]\n",
    "test_prc22=[]\n",
    "#mol2vec和全部descriptor的混合\n",
    "model = word2vec.Word2Vec.load('./mol2vec-master/examples/models/model_300dim.pkl')\n",
    "for i in range(10):\n",
    "    data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "    data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    data['Descriptors']=data['mol'].apply(get_fps)\n",
    "    data['Descriptors'].fillna(data.mean())\n",
    "    data['sentence'] = data.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    data['mol2vec'] = [DfVec(x) for x in sentences2vec(data['sentence'], model, unseen='UNK')]\n",
    "    \n",
    "    X_mol = np.array([x.vec for x in data['mol2vec']])\n",
    "    X_mol = pd.DataFrame(X_mol)\n",
    "\n",
    "    ytrain=data['activity']\n",
    "    xtrain=data.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "    X_de = np.array([x for x in data['Descriptors']])\n",
    "    X_de = pd.DataFrame(X_de)\n",
    "    xtrain = pd.concat((X_de, X_mol), axis=1)\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "    test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    test['Descriptors']=test['mol'].apply(get_fps)\n",
    "    test['Descriptors'].fillna(test.mean())\n",
    "    test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "    ytest=test['activity']\n",
    "    xtest=test.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "    X_mol2 = np.array([x.vec for x in test['mol2vec']])\n",
    "    X_mol2 = pd.DataFrame(X_mol2)\n",
    "    X_de2 = np.array([x for x in test['Descriptors']])\n",
    "    X_de2 = pd.DataFrame(X_de2)\n",
    "    xtest = pd.concat((X_de2, X_mol2), axis=1)\n",
    "    xtest=np.nan_to_num(xtest)\n",
    "    X_train = StandardScaler().fit_transform(xtrain)\n",
    "    X_test = StandardScaler().fit_transform(xtest)\n",
    "    #lr = BlendEnsemble(verbose=2)\n",
    "    lr2=MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=1)\n",
    "    lr = RandomForestClassifier(max_features='auto')\n",
    "    lr.fit(X_train, ytrain)\n",
    "    lr2.fit(X_train, ytrain)\n",
    "    predict_prob_y = lr.predict_proba(X_test)\n",
    "    predict_prob_y2 = lr2.predict_proba(X_test)\n",
    "    test_roc2.append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "    test_roc22.append(metrics.roc_auc_score(ytest,predict_prob_y2[:,1]))\n",
    "    fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "    fpr2, tpr2, thresholds=(precision_recall_curve(ytest,predict_prob_y2[:,1]))\n",
    "    test_prc2.append(metrics.auc( tpr,fpr))\n",
    "    test_prc22.append(metrics.auc(tpr2,fpr2))\n",
    "    \n",
    "    del lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8020580292519908\n",
      "0.3954048849638902\n",
      "0.7638728763498511\n",
      "0.4542323642203514\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_roc2))\n",
    "print(np.mean(test_prc2))\n",
    "print(np.mean(test_roc22))\n",
    "print(np.mean(test_prc22))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三个实验，不同分子指纹的分类效果，使用随机森林和mlp作为验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各类指纹功效\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "roc = {}\n",
    "prc={}     \n",
    "   \n",
    "for f in fps:\n",
    "\n",
    "    roc[f] = {}\n",
    "    prc[f]={}\n",
    "    for m in models:\n",
    "        roc[f][m]=[]\n",
    "        prc[f][m]=[]\n",
    "        for i in range(10):\n",
    "            data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "            data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "            test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "            test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "            fps = {\"ECFP4\": data['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048)),\n",
    "           \"ECFP6\": data['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=3, nBits=2048)),\n",
    "           \"RDKFP\":data['mol'].apply(lambda m: AllChem.RDKFingerprint(m, fpSize=2048))}\n",
    "\n",
    "\n",
    "            ytrain=np.array(data['activity'])\n",
    "            fpstest = {\"ECFP4\": test['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048)),\n",
    "           \"ECFP6\": test['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=3, nBits=2048)),\n",
    "           \"RDKFP\":test['mol'].apply(lambda m: AllChem.RDKFingerprint(m, fpSize=2048))}\n",
    "            xtrain = np.array(fps[f].tolist())\n",
    "            xtest = np.array(fpstest[f].tolist())\n",
    "            # Default models\n",
    "            models = {\"rf\": RandomForestClassifier(max_features='auto'),\n",
    "              \"nnet\":MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=1) }\n",
    "            ytest=np.array(test['activity'])\n",
    "\n",
    "            models[m].fit(xtrain, ytrain)\n",
    "            #scores[f][m + \"_r2_train\"] = models[m].score(X_train, y_train)\n",
    "            predict_prob_y = models[m].predict_proba(xtest)\n",
    "            roc[f][m].append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "            fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "            prc[f][m].append(metrics.auc( tpr,fpr))\n",
    "\n",
    "for i in models:\n",
    "    for j in fps:\n",
    "        roc[i][j]=np.mean(roc[i][j])\n",
    "        prc[j][i]=np.mean(prc[j][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下打印的是三种指纹对应的随机森林和多层感知机的分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             rf      nnet\n",
      "ECFP4   0.77559  0.694632\n",
      "ECFP6  0.768347  0.711039\n",
      "RDKFP  0.774321  0.721036\n",
      "             rf      nnet\n",
      "ECFP4  0.415972  0.330636\n",
      "ECFP6  0.395199  0.315473\n",
      "RDKFP  0.397939  0.398258\n"
     ]
    }
   ],
   "source": [
    "print(roc)\n",
    "print(prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四个实验混合mol2vec和所有描述符和指纹进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner,BlendEnsemble\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "test_auc4=[]\n",
    "test_prc4=[]\n",
    "#mol2vec和全部descriptor和指纹的混合\n",
    "\n",
    "for i in range(10):\n",
    "    data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "    data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    data['Descriptors']=data['mol'].apply(get_fps)\n",
    "    data['Descriptors'].fillna(data.mean())\n",
    "    data['sentence'] = data.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    data['mol2vec'] = [DfVec(x) for x in sentences2vec(data['sentence'], model, unseen='UNK')]\n",
    "    data['fingerprint']=data['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048))\n",
    "    xf=np.array(data['fingerprint'].tolist())\n",
    "    xf=pd.DataFrame(xf)\n",
    "    X_mol = np.array([x.vec for x in data['mol2vec']])\n",
    "    X_mol = pd.DataFrame(X_mol)\n",
    "    \n",
    "    ytrain=data['activity']\n",
    "    xtrain=data.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "    X_de = np.array([x for x in data['Descriptors']])\n",
    "    X_de = pd.DataFrame(X_de)\n",
    "    xtrain = pd.concat((X_de, X_mol,xf), axis=1)\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "    test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    test['Descriptors']=test['mol'].apply(get_fps)\n",
    "    test['Descriptors'].fillna(test.mean())\n",
    "    test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "    test['fingerprint']=test['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048))\n",
    "    xf2=np.array(test['fingerprint'].tolist())\n",
    "    xf2=pd.DataFrame(xf2)\n",
    "    ytest=test['activity']\n",
    "    xtest=test.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "    X_mol2 = np.array([x.vec for x in test['mol2vec']])\n",
    "    X_mol2 = pd.DataFrame(X_mol2)\n",
    "    X_de2 = np.array([x for x in test['Descriptors']])\n",
    "    X_de2 = pd.DataFrame(X_de2)\n",
    "    xtest = pd.concat((X_de2, X_mol2,xf2), axis=1)\n",
    "    xtest=np.nan_to_num(xtest)\n",
    "    X_train = StandardScaler().fit_transform(xtrain)\n",
    "    X_test = StandardScaler().fit_transform(xtest)\n",
    "    #lr = BlendEnsemble(verbose=2)\n",
    "    #lr.add([RandomForestClassifier(max_features='auto'),MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=1)],)\n",
    "    #lr.add_meta(LogisticRegression(solver='lbfgs', multi_class='auto'))\n",
    "    #lr=MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=1)\n",
    "    lr = RandomForestClassifier(max_features='auto')\n",
    "    lr.fit(X_train, ytrain)\n",
    "    predict_prob_y = lr.predict_proba(X_test)\n",
    "    test_auc4.append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "    fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "    test_prc4.append(metrics.auc( tpr,fpr))\n",
    "    \n",
    "    del lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以看到混合了所有特征后的模型准确度最高的，达到85%的roc值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8502830356668817\n",
      "0.43819971569910193\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_auc4))\n",
    "print(np.mean(test_prc4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四个实验，不同采样结果加随机森林分类器的对比\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠采样对比\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler,\n",
    "                                     NearMiss,\n",
    "                                     InstanceHardnessThreshold,\n",
    "                                     CondensedNearestNeighbour,\n",
    "                                     EditedNearestNeighbours,\n",
    "                                     RepeatedEditedNearestNeighbours,\n",
    "                                     AllKNN,\n",
    "                                     NeighbourhoodCleaningRule,\n",
    "                                     OneSidedSelection)\n",
    "test_auc5={}\n",
    "test_prc5={}\n",
    "#mol2vec和全部descriptor的混合的平衡采样\n",
    "sampling={}\n",
    "for i in range(10):\n",
    "    data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "    data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    data['Descriptors']=data['mol'].apply(get_fps)\n",
    "    data['Descriptors'].fillna(data.mean())\n",
    "    data['sentence'] = data.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    data['mol2vec'] = [DfVec(x) for x in sentences2vec(data['sentence'], model, unseen='UNK')]\n",
    "    \n",
    "    X_mol = np.array([x.vec for x in data['mol2vec']])\n",
    "    X_mol = pd.DataFrame(X_mol)\n",
    "\n",
    "    ytrain=np.array(data['activity'])\n",
    "    xtrain=np.zeros([X_mol.shape[0],279])\n",
    "    for p in range(X_mol.shape[0]):\n",
    "        xtrain[p,:]=data['Descriptors'][p]\n",
    "    \n",
    "    xtrain = np.concatenate((xtrain, X_mol), axis=1)\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    xtrain[xtrain >= np.finfo(np.float32).max]=np.finfo(np.float32).max\n",
    "    xtrain=np.array(xtrain)\n",
    "    xtrain.shape\n",
    "    sampling={'ClusterCentroids':ClusterCentroids().fit_resample(xtrain, ytrain),'RandomUnderSampler':RandomUnderSampler().fit_resample(xtrain, ytrain),\n",
    "             'NearMiss':NearMiss().fit_resample(xtrain, ytrain),'InstanceHardnessThreshold':InstanceHardnessThreshold().fit_resample(xtrain, ytrain),\n",
    "              'CondensedNearestNeighbour':CondensedNearestNeighbour().fit_resample(xtrain, ytrain),'SMOTE':SMOTE().fit_resample(xtrain, ytrain)\n",
    "             }\n",
    "    test_auc5[str(i)]=[]\n",
    "    test_prc5[str(i)]=[]\n",
    "    for s in sampling:\n",
    "        \n",
    "        X_resampled, Y_resampled = sampling[s]\n",
    "        test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "        test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "        test['Descriptors']=test['mol'].apply(get_fps)\n",
    "        test['Descriptors'].fillna(test.mean())\n",
    "        test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "        test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "        ytest=test['activity']\n",
    "        xtest=test.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "        X_mol2 = np.array([x.vec for x in test['mol2vec']])\n",
    "        X_mol2 = pd.DataFrame(X_mol2)\n",
    "        X_de2 = np.array([x for x in test['Descriptors']])\n",
    "        X_de2 = pd.DataFrame(X_de2)\n",
    "        xtest = pd.concat((X_de2, X_mol2), axis=1)\n",
    "        xtest=np.nan_to_num(xtest)\n",
    "\n",
    "        X_train = StandardScaler().fit_transform(X_resampled)\n",
    "        X_test = StandardScaler().fit_transform(xtest)\n",
    "\n",
    "        lr = RandomForestClassifier(max_features='auto')\n",
    "        lr.fit(X_train,Y_resampled)\n",
    "        predict_prob_y = lr.predict_proba(X_test)\n",
    "        test_auc5[str(i)].append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "        fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "        test_prc5[str(i)].append(metrics.auc( tpr,fpr))\n",
    "        \n",
    "test_auc5\n",
    "\n",
    "avroc=np.zeros(6)\n",
    "pvroc=np.zeros(6)\n",
    "for j in range(6):\n",
    "    for i in range(10):\n",
    "        avroc[j]+=test_auc5[str(i)][j]\n",
    "        pvroc[j]+=test_prc5[str(i)][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52863248 0.7603706  0.61471868 0.76443984 0.7965451  0.75578389]\n",
      "[0.04164929 0.32034436 0.09140746 0.09645921 0.40275834 0.35208798]\n"
     ]
    }
   ],
   "source": [
    "print(avroc*0.1)\n",
    "print(pvroc*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第五个实验不同采样方法加xgboost分类器\n",
    "由于在上面的采样实验中，所有采样方法配合随机森林使用，都没有超过未采样结果0.80，所以在这里我换用xgboost分类器再次尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "\n",
    "from imblearn.under_sampling import (NearMiss)\n",
    "test_auc5={}\n",
    "test_prc5={}\n",
    "#mol2vec和全部descriptor的混合的平衡采样\n",
    "sampling={}\n",
    "for i in range(10):\n",
    "    data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "    data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    data['Descriptors']=data['mol'].apply(get_fps)\n",
    "    data['Descriptors'].fillna(data.mean())\n",
    "    data['sentence'] = data.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    data['mol2vec'] = [DfVec(x) for x in sentences2vec(data['sentence'], model, unseen='UNK')]\n",
    "    \n",
    "    X_mol = np.array([x.vec for x in data['mol2vec']])\n",
    "    X_mol = pd.DataFrame(X_mol)\n",
    "\n",
    "    ytrain=np.array(data['activity'])\n",
    "    xtrain=np.zeros([X_mol.shape[0],279])\n",
    "    for p in range(X_mol.shape[0]):\n",
    "        xtrain[p,:]=data['Descriptors'][p]\n",
    "    \n",
    "    xtrain = np.concatenate((xtrain, X_mol), axis=1)\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    xtrain[xtrain >= np.finfo(np.float32).max]=np.finfo(np.float32).max\n",
    "    xtrain=np.array(xtrain)\n",
    "    xtrain.shape\n",
    "    sampling={'SMOTE': SMOTE(), \n",
    "              'ADASYN':  ADASYN(random_state=42), \n",
    "              'NearMiss':NearMiss(version=3), 'SMOTE+ENN':  SMOTEENN(), 'SMOTE+Tomek':  SMOTETomek()\n",
    "             }\n",
    "    test_auc5[str(i)]=[]\n",
    "    test_prc5[str(i)]=[]\n",
    "    for s in sampling:\n",
    "        \n",
    "        X_resampled, Y_resampled = sampling[s].fit_resample(xtrain, ytrain)\n",
    "        test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "        test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "        test['Descriptors']=test['mol'].apply(get_fps)\n",
    "        test['Descriptors'].fillna(test.mean())\n",
    "        test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "        test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "        ytest=test['activity']\n",
    "        xtest=test.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "        X_mol2 = np.array([x.vec for x in test['mol2vec']])\n",
    "        X_mol2 = pd.DataFrame(X_mol2)\n",
    "        X_de2 = np.array([x for x in test['Descriptors']])\n",
    "        X_de2 = pd.DataFrame(X_de2)\n",
    "        xtest = pd.concat((X_de2, X_mol2), axis=1)\n",
    "        xtest=np.nan_to_num(xtest)\n",
    "\n",
    "        X_train = StandardScaler().fit_transform(X_resampled)\n",
    "        X_test = StandardScaler().fit_transform(xtest)\n",
    "\n",
    "        lr = XGBClassifier(random_state = 42, n_jobs = -1)\n",
    "        lr.fit(X_train,Y_resampled)\n",
    "        predict_prob_y = lr.predict_proba(X_test)\n",
    "        test_auc5[str(i)].append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "        fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "        test_prc5[str(i)].append(metrics.auc( tpr,fpr))\n",
    "        \n",
    "test_auc5\n",
    "\n",
    "avroc=np.zeros(5)\n",
    "pvroc=np.zeros(5)\n",
    "for j in range(5):\n",
    "    for i in range(10):\n",
    "        avroc[j]+=test_auc5[str(i)][j]\n",
    "        pvroc[j]+=test_prc5[str(i)][j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从结果看到尽管换用xgboost分类器，但是结果还是普遍下降（xgboost不采样roc=0.81），这里让我非常疑惑，尽管在这里的两种方法都没有使得roc结果变好，但是在我们组的图网络中采样结果却让结果变好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72078436 0.69827815 0.64164957 0.698253   0.71224579]\n",
      "[0.23505263 0.27909201 0.09452962 0.24142565 0.20777271]\n"
     ]
    }
   ],
   "source": [
    "print(avroc*0.1)\n",
    "print(pvroc*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第六个实验集成投票法，从第一个实验中就可以看到，即使用了同样的特征在不同的分类方法下每折的表现也十分不同，所以尝试使用投票法对结果进行估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45195578\n",
      "Iteration 2, loss = 0.04759425\n",
      "Iteration 3, loss = 0.01449684\n",
      "Iteration 4, loss = 0.00806241\n",
      "Iteration 5, loss = 0.00455797\n",
      "Iteration 6, loss = 0.00285997\n",
      "Iteration 7, loss = 0.00243215\n",
      "Iteration 8, loss = 0.00198261\n",
      "Iteration 9, loss = 0.00182723\n",
      "Iteration 10, loss = 0.00180385\n",
      "Iteration 11, loss = 0.00169753\n",
      "Iteration 12, loss = 0.00162357\n",
      "Iteration 13, loss = 0.00139233\n",
      "Iteration 14, loss = 0.00131881\n",
      "Iteration 15, loss = 0.00129868\n",
      "Iteration 16, loss = 0.00118180\n",
      "Iteration 17, loss = 0.00113718\n",
      "Iteration 18, loss = 0.00108286\n",
      "Iteration 19, loss = 0.00105293\n",
      "Iteration 20, loss = 0.00111642\n",
      "Iteration 21, loss = 0.00096632\n",
      "Iteration 22, loss = 0.00088769\n",
      "Iteration 23, loss = 0.00108628\n",
      "Iteration 24, loss = 0.00081393\n",
      "Iteration 25, loss = 0.00091593\n",
      "Iteration 26, loss = 0.00087110\n",
      "Iteration 27, loss = 0.00077899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61398380\n",
      "Iteration 2, loss = 0.05775985\n",
      "Iteration 3, loss = 0.02151706\n",
      "Iteration 4, loss = 0.01211875\n",
      "Iteration 5, loss = 0.00807905\n",
      "Iteration 6, loss = 0.00532307\n",
      "Iteration 7, loss = 0.00380901\n",
      "Iteration 8, loss = 0.00302143\n",
      "Iteration 9, loss = 0.00290581\n",
      "Iteration 10, loss = 0.00299405\n",
      "Iteration 11, loss = 0.00248661\n",
      "Iteration 12, loss = 0.00177510\n",
      "Iteration 13, loss = 0.00187069\n",
      "Iteration 14, loss = 0.00174956\n",
      "Iteration 15, loss = 0.00183503\n",
      "Iteration 16, loss = 0.00155170\n",
      "Iteration 17, loss = 0.00139303\n",
      "Iteration 18, loss = 0.00134486\n",
      "Iteration 19, loss = 0.00121807\n",
      "Iteration 20, loss = 0.00125279\n",
      "Iteration 21, loss = 0.00109759\n",
      "Iteration 22, loss = 0.00109090\n",
      "Iteration 23, loss = 0.00107445\n",
      "Iteration 24, loss = 0.00097733\n",
      "Iteration 25, loss = 0.00096866\n",
      "Iteration 26, loss = 0.00091608\n",
      "Iteration 27, loss = 0.00088443\n",
      "Iteration 28, loss = 0.00083568\n",
      "Iteration 29, loss = 0.00083366\n",
      "Iteration 30, loss = 0.00076874\n",
      "Iteration 31, loss = 0.00076563\n",
      "Iteration 32, loss = 0.00075335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38269078\n",
      "Iteration 2, loss = 0.04258485\n",
      "Iteration 3, loss = 0.01688729\n",
      "Iteration 4, loss = 0.00923861\n",
      "Iteration 5, loss = 0.00622415\n",
      "Iteration 6, loss = 0.00324177\n",
      "Iteration 7, loss = 0.00274197\n",
      "Iteration 8, loss = 0.00255018\n",
      "Iteration 9, loss = 0.00218399\n",
      "Iteration 10, loss = 0.00200809\n",
      "Iteration 11, loss = 0.00224103\n",
      "Iteration 12, loss = 0.00191300\n",
      "Iteration 13, loss = 0.00145760\n",
      "Iteration 14, loss = 0.00144607\n",
      "Iteration 15, loss = 0.00128371\n",
      "Iteration 16, loss = 0.00125625\n",
      "Iteration 17, loss = 0.00125059\n",
      "Iteration 18, loss = 0.00098367\n",
      "Iteration 19, loss = 0.00148354\n",
      "Iteration 20, loss = 0.00128943\n",
      "Iteration 21, loss = 0.00102287\n",
      "Iteration 22, loss = 0.00092036\n",
      "Iteration 23, loss = 0.00102534\n",
      "Iteration 24, loss = 0.00085622\n",
      "Iteration 25, loss = 0.00106247\n",
      "Iteration 26, loss = 0.00076609\n",
      "Iteration 27, loss = 0.00086387\n",
      "Iteration 28, loss = 0.00078352\n",
      "Iteration 29, loss = 0.00088658\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27780672\n",
      "Iteration 2, loss = 0.03418007\n",
      "Iteration 3, loss = 0.01356445\n",
      "Iteration 4, loss = 0.00733622\n",
      "Iteration 5, loss = 0.00340278\n",
      "Iteration 6, loss = 0.00440660\n",
      "Iteration 7, loss = 0.00407671\n",
      "Iteration 8, loss = 0.00248785\n",
      "Iteration 9, loss = 0.00178934\n",
      "Iteration 10, loss = 0.00216498\n",
      "Iteration 11, loss = 0.00136260\n",
      "Iteration 12, loss = 0.00112572\n",
      "Iteration 13, loss = 0.00184036\n",
      "Iteration 14, loss = 0.00088265\n",
      "Iteration 15, loss = 0.00245313\n",
      "Iteration 16, loss = 0.00133113\n",
      "Iteration 17, loss = 0.00095243\n",
      "Iteration 18, loss = 0.00185325\n",
      "Iteration 19, loss = 0.00100564\n",
      "Iteration 20, loss = 0.00159746\n",
      "Iteration 21, loss = 0.00113058\n",
      "Iteration 22, loss = 0.00137798\n",
      "Iteration 23, loss = 0.00175985\n",
      "Iteration 24, loss = 0.00170296\n",
      "Iteration 25, loss = 0.00070564\n",
      "Iteration 26, loss = 0.00063628\n",
      "Iteration 27, loss = 0.00077284\n",
      "Iteration 28, loss = 0.00054546\n",
      "Iteration 29, loss = 0.00054645\n",
      "Iteration 30, loss = 0.00051591\n",
      "Iteration 31, loss = 0.00052453\n",
      "Iteration 32, loss = 0.00046538\n",
      "Iteration 33, loss = 0.00051138\n",
      "Iteration 34, loss = 0.00046115\n",
      "Iteration 35, loss = 0.00047238\n",
      "Iteration 36, loss = 0.00042089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40085253\n",
      "Iteration 2, loss = 0.04428537\n",
      "Iteration 3, loss = 0.01796574\n",
      "Iteration 4, loss = 0.01025173\n",
      "Iteration 5, loss = 0.00592994\n",
      "Iteration 6, loss = 0.00370470\n",
      "Iteration 7, loss = 0.00315792\n",
      "Iteration 8, loss = 0.00266511\n",
      "Iteration 9, loss = 0.00238756\n",
      "Iteration 10, loss = 0.00217814\n",
      "Iteration 11, loss = 0.00212693\n",
      "Iteration 12, loss = 0.00183040\n",
      "Iteration 13, loss = 0.00175072\n",
      "Iteration 14, loss = 0.00160121\n",
      "Iteration 15, loss = 0.00160712\n",
      "Iteration 16, loss = 0.00154297\n",
      "Iteration 17, loss = 0.00165149\n",
      "Iteration 18, loss = 0.00154633\n",
      "Iteration 19, loss = 0.00151144\n",
      "Iteration 20, loss = 0.00169652\n",
      "Iteration 21, loss = 0.00122912\n",
      "Iteration 22, loss = 0.00104986\n",
      "Iteration 23, loss = 0.00105151\n",
      "Iteration 24, loss = 0.00096162\n",
      "Iteration 25, loss = 0.00098902\n",
      "Iteration 26, loss = 0.00090536\n",
      "Iteration 27, loss = 0.00092285\n",
      "Iteration 28, loss = 0.00086783\n",
      "Iteration 29, loss = 0.00087593\n",
      "Iteration 30, loss = 0.00122977\n",
      "Iteration 31, loss = 0.00093983\n",
      "Iteration 32, loss = 0.00074602\n",
      "Iteration 33, loss = 0.00085894\n",
      "Iteration 34, loss = 0.00075437\n",
      "Iteration 35, loss = 0.00080417\n",
      "Iteration 36, loss = 0.00061734\n",
      "Iteration 37, loss = 0.00058704\n",
      "Iteration 38, loss = 0.00064618\n",
      "Iteration 39, loss = 0.00057573\n",
      "Iteration 40, loss = 0.00056115\n",
      "Iteration 41, loss = 0.00054553\n",
      "Iteration 42, loss = 0.00051002\n",
      "Iteration 43, loss = 0.00053239\n",
      "Iteration 44, loss = 0.00050097\n",
      "Iteration 45, loss = 0.00048656\n",
      "Iteration 46, loss = 0.00045639\n",
      "Iteration 47, loss = 0.00045382\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66904281\n",
      "Iteration 2, loss = 0.05882049\n",
      "Iteration 3, loss = 0.01763929\n",
      "Iteration 4, loss = 0.01097843\n",
      "Iteration 5, loss = 0.00834414\n",
      "Iteration 6, loss = 0.00693229\n",
      "Iteration 7, loss = 0.00458692\n",
      "Iteration 8, loss = 0.00367357\n",
      "Iteration 9, loss = 0.00228528\n",
      "Iteration 10, loss = 0.00205352\n",
      "Iteration 11, loss = 0.00176278\n",
      "Iteration 12, loss = 0.00159311\n",
      "Iteration 13, loss = 0.00140327\n",
      "Iteration 14, loss = 0.00129056\n",
      "Iteration 15, loss = 0.00119170\n",
      "Iteration 16, loss = 0.00112097\n",
      "Iteration 17, loss = 0.00105450\n",
      "Iteration 18, loss = 0.00099012\n",
      "Iteration 19, loss = 0.00092903\n",
      "Iteration 20, loss = 0.00090360\n",
      "Iteration 21, loss = 0.00083156\n",
      "Iteration 22, loss = 0.00079786\n",
      "Iteration 23, loss = 0.00074932\n",
      "Iteration 24, loss = 0.00072075\n",
      "Iteration 25, loss = 0.00070030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56525418\n",
      "Iteration 2, loss = 0.05392844\n",
      "Iteration 3, loss = 0.01816887\n",
      "Iteration 4, loss = 0.00952628\n",
      "Iteration 5, loss = 0.00496206\n",
      "Iteration 6, loss = 0.00306672\n",
      "Iteration 7, loss = 0.00227831\n",
      "Iteration 8, loss = 0.00226375\n",
      "Iteration 9, loss = 0.00182079\n",
      "Iteration 10, loss = 0.00179515\n",
      "Iteration 11, loss = 0.00140080\n",
      "Iteration 12, loss = 0.00132048\n",
      "Iteration 13, loss = 0.00122763\n",
      "Iteration 14, loss = 0.00112251\n",
      "Iteration 15, loss = 0.00105259\n",
      "Iteration 16, loss = 0.00098587\n",
      "Iteration 17, loss = 0.00092839\n",
      "Iteration 18, loss = 0.00087857\n",
      "Iteration 19, loss = 0.00083623\n",
      "Iteration 20, loss = 0.00079294\n",
      "Iteration 21, loss = 0.00075009\n",
      "Iteration 22, loss = 0.00071597\n",
      "Iteration 23, loss = 0.00068308\n",
      "Iteration 24, loss = 0.00065229\n",
      "Iteration 25, loss = 0.00062735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46456430\n",
      "Iteration 2, loss = 0.05265618\n",
      "Iteration 3, loss = 0.01721221\n",
      "Iteration 4, loss = 0.00698001\n",
      "Iteration 5, loss = 0.00370805\n",
      "Iteration 6, loss = 0.00281233\n",
      "Iteration 7, loss = 0.00235705\n",
      "Iteration 8, loss = 0.00219866\n",
      "Iteration 9, loss = 0.00231585\n",
      "Iteration 10, loss = 0.00180288\n",
      "Iteration 11, loss = 0.00202369\n",
      "Iteration 12, loss = 0.00196535\n",
      "Iteration 13, loss = 0.00179177\n",
      "Iteration 14, loss = 0.00142925\n",
      "Iteration 15, loss = 0.00137994\n",
      "Iteration 16, loss = 0.00126531\n",
      "Iteration 17, loss = 0.00117457\n",
      "Iteration 18, loss = 0.00124827\n",
      "Iteration 19, loss = 0.00102232\n",
      "Iteration 20, loss = 0.00174874\n",
      "Iteration 21, loss = 0.00081574\n",
      "Iteration 22, loss = 0.00140119\n",
      "Iteration 23, loss = 0.00092947\n",
      "Iteration 24, loss = 0.00117534\n",
      "Iteration 25, loss = 0.00072807\n",
      "Iteration 26, loss = 0.00117972\n",
      "Iteration 27, loss = 0.00090270\n",
      "Iteration 28, loss = 0.00186616\n",
      "Iteration 29, loss = 0.00076612\n",
      "Iteration 30, loss = 0.00200910\n",
      "Iteration 31, loss = 0.00128200\n",
      "Iteration 32, loss = 0.00176290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84194608\n",
      "Iteration 2, loss = 0.08664520\n",
      "Iteration 3, loss = 0.02404029\n",
      "Iteration 4, loss = 0.01270756\n",
      "Iteration 5, loss = 0.00791456\n",
      "Iteration 6, loss = 0.00557379\n",
      "Iteration 7, loss = 0.00515611\n",
      "Iteration 8, loss = 0.00365391\n",
      "Iteration 9, loss = 0.00287930\n",
      "Iteration 10, loss = 0.00273266\n",
      "Iteration 11, loss = 0.00243082\n",
      "Iteration 12, loss = 0.00230337\n",
      "Iteration 13, loss = 0.00201424\n",
      "Iteration 14, loss = 0.00200357\n",
      "Iteration 15, loss = 0.00187653\n",
      "Iteration 16, loss = 0.00176555\n",
      "Iteration 17, loss = 0.00159459\n",
      "Iteration 18, loss = 0.00154023\n",
      "Iteration 19, loss = 0.00158196\n",
      "Iteration 20, loss = 0.00180276\n",
      "Iteration 21, loss = 0.00168784\n",
      "Iteration 22, loss = 0.00125313\n",
      "Iteration 23, loss = 0.00146678\n",
      "Iteration 24, loss = 0.00111809\n",
      "Iteration 25, loss = 0.00114501\n",
      "Iteration 26, loss = 0.00110596\n",
      "Iteration 27, loss = 0.00101091\n",
      "Iteration 28, loss = 0.00094958\n",
      "Iteration 29, loss = 0.00093047\n",
      "Iteration 30, loss = 0.00087252\n",
      "Iteration 31, loss = 0.00084125\n",
      "Iteration 32, loss = 0.00088125\n",
      "Iteration 33, loss = 0.00079013\n",
      "Iteration 34, loss = 0.00086320\n",
      "Iteration 35, loss = 0.00070918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46398083\n",
      "Iteration 2, loss = 0.04538643\n",
      "Iteration 3, loss = 0.01636655\n",
      "Iteration 4, loss = 0.00881287\n",
      "Iteration 5, loss = 0.00487773\n",
      "Iteration 6, loss = 0.00395453\n",
      "Iteration 7, loss = 0.00295464\n",
      "Iteration 8, loss = 0.00231038\n",
      "Iteration 9, loss = 0.00208686\n",
      "Iteration 10, loss = 0.00190337\n",
      "Iteration 11, loss = 0.00163060\n",
      "Iteration 12, loss = 0.00170006\n",
      "Iteration 13, loss = 0.00148887\n",
      "Iteration 14, loss = 0.00142310\n",
      "Iteration 15, loss = 0.00146492\n",
      "Iteration 16, loss = 0.00150801\n",
      "Iteration 17, loss = 0.00151634\n",
      "Iteration 18, loss = 0.00141254\n",
      "Iteration 19, loss = 0.00127180\n",
      "Iteration 20, loss = 0.00112910\n",
      "Iteration 21, loss = 0.00092112\n",
      "Iteration 22, loss = 0.00165116\n",
      "Iteration 23, loss = 0.00112142\n",
      "Iteration 24, loss = 0.00130432\n",
      "Iteration 25, loss = 0.00083659\n",
      "Iteration 26, loss = 0.00071105\n",
      "Iteration 27, loss = 0.00079291\n",
      "Iteration 28, loss = 0.00070413\n",
      "Iteration 29, loss = 0.00065005\n",
      "Iteration 30, loss = 0.00071073\n",
      "Iteration 31, loss = 0.00060157\n",
      "Iteration 32, loss = 0.00065506\n",
      "Iteration 33, loss = 0.00057084\n",
      "Iteration 34, loss = 0.00055452\n",
      "Iteration 35, loss = 0.00054288\n",
      "Iteration 36, loss = 0.00053447\n",
      "Iteration 37, loss = 0.00051579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner,BlendEnsemble\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "test_auc6=[]\n",
    "test_prc6=[]\n",
    "#mol2vec和全部descriptor和指纹的混合\n",
    "\n",
    "for i in range(10):\n",
    "    data =pd.read_csv('fold_'+str(i)+'/train.csv')\n",
    "    data['mol'] = data['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    data['Descriptors']=data['mol'].apply(get_fps)\n",
    "    data['Descriptors'].fillna(data.mean())\n",
    "    data['sentence'] = data.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    data['mol2vec'] = [DfVec(x) for x in sentences2vec(data['sentence'], model, unseen='UNK')]\n",
    "    data['fingerprint']=data['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048))\n",
    "    xf=np.array(data['fingerprint'].tolist())\n",
    "    xf=pd.DataFrame(xf)\n",
    "    X_mol = np.array([x.vec for x in data['mol2vec']])\n",
    "    X_mol = pd.DataFrame(X_mol)\n",
    "    \n",
    "    ytrain=data['activity']\n",
    "    xtrain=data.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "    X_de = np.array([x for x in data['Descriptors']])\n",
    "    X_de = pd.DataFrame(X_de)\n",
    "    xtrain = pd.concat((X_de, X_mol,xf), axis=1)\n",
    "    xtrain=np.nan_to_num(xtrain)\n",
    "    test =pd.read_csv('fold_'+str(i)+'/test.csv')\n",
    "    test['mol'] = test['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "    test['Descriptors']=test['mol'].apply(get_fps)\n",
    "    test['Descriptors'].fillna(test.mean())\n",
    "    test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "    test['fingerprint']=test['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=2048))\n",
    "    xf2=np.array(test['fingerprint'].tolist())\n",
    "    xf2=pd.DataFrame(xf2)\n",
    "    ytest=test['activity']\n",
    "    xtest=test.drop(columns=['smiles', 'activity','mol','sentence','mol2vec'])\n",
    "    X_mol2 = np.array([x.vec for x in test['mol2vec']])\n",
    "    X_mol2 = pd.DataFrame(X_mol2)\n",
    "    X_de2 = np.array([x for x in test['Descriptors']])\n",
    "    X_de2 = pd.DataFrame(X_de2)\n",
    "    xtest = pd.concat((X_de2, X_mol2,xf2), axis=1)\n",
    "    xtest=np.nan_to_num(xtest)\n",
    "    X_train = StandardScaler().fit_transform(xtrain)\n",
    "    X_test = StandardScaler().fit_transform(xtest)\n",
    "    #lr = BlendEnsemble(verbose=2)\n",
    "    #lr.add([RandomForestClassifier(max_features='auto'),MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=1)],)\n",
    "    #lr.add_meta(LogisticRegression(solver='lbfgs', multi_class='auto'))\n",
    "    lr2=MLPClassifier(activation='relu', solver='adam', alpha=0.0001,verbose=1)\n",
    "    lr = RandomForestClassifier(max_features='auto')\n",
    "    lr3 = XGBClassifier(random_state = 42, n_jobs = -1)\n",
    "    lr.fit(X_train, ytrain)\n",
    "    lr2.fit(X_train, ytrain)\n",
    "    lr3.fit(X_train, ytrain)\n",
    "    predict_prob_y = (lr.predict_proba(X_test)+lr2.predict_proba(X_test)+lr3.predict_proba(X_test))/3\n",
    "    test_auc6.append(metrics.roc_auc_score(ytest,predict_prob_y[:,1]))\n",
    "    fpr, tpr, thresholds=(precision_recall_curve(ytest,predict_prob_y[:,1]))\n",
    "    test_prc6.append(metrics.auc( tpr,fpr))\n",
    "    \n",
    "    del lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4650270236320745"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_prc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
